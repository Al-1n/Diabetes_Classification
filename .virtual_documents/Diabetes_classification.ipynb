#!pip install optuna
#!pip install jupyterlab-optuna
#!pip install optuna-fast-fanova gunicorn
#!pip install imbalanced-learn
#!pip install hyperopt
#!pip install lightgbm
#!pip install catboost


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import log_loss, roc_auc_score
from sklearn.metrics import roc_curve, precision_recall_curve
import matplotlib.pyplot as plt
import optuna


df = pd.read_csv("./Data/diabetes.csv", sep = ",")

df.head()





#Verify that all columns have numeric types
df.dtypes


#Check the target variable label values
df["Outcome"].value_counts()


#Get the summary sttistics for all the columns
df.describe()


df.hist(bins = 30, figsize = (10, 10), color = 'orange')

plt.show()





print(df[df['Glucose'] == 0].shape[0])
print(df[df['Insulin'] == 0].shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BloodPressure"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0) & (df["BloodPressure"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Outcome"] == 1)]).shape[0])
print(pd.DataFrame(df[(df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0])





print(df[df["Outcome"] == 0]["Insulin"].describe())

print()

print(df[df["Outcome"] == 1]["Insulin"].describe())

print()

print(df["Insulin"].describe())

print()

print(np.median(df[(df["Outcome"] == 1) & (df["Insulin"] != 0)]["Insulin"]))


df["Insulin"].hist(bins = 30)


import numpy as np

df.loc[(df["Outcome"] == 1) & (df["Insulin"] == 0), "Insulin"] = int(np.mean(df["Insulin"]))

df.loc[(df["Outcome"] == 0) & (df["Insulin"] == 0), "Insulin"] = int(np.median(df["Insulin"]))


print(df[df["Outcome"] == 0]["Insulin"].describe())

print()

print(df[df["Outcome"] == 1]["Insulin"].describe())

print()

print(df["Insulin"].describe())

print()

print(np.median(df[df["Outcome"] == 1]["Insulin"]))




df["Insulin"].hist(bins = 30)


print(df[df['Glucose'] == 0].shape[0])
print(df[df['Insulin'] == 0].shape[0])
print(df[df['BMI'] == 0].shape[0])
print(df[df['BloodPressure'] == 0].shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BloodPressure"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0) & (df["BloodPressure"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Outcome"] == 1)]).shape[0])
print(pd.DataFrame(df[(df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0])


print(df[df["Outcome"] == 0]["BMI"].describe())

print()

print(df[df["Outcome"] == 1]["BMI"].describe())

print()

print(df["BMI"].describe())

print()

print(df[df["Outcome"] == 0]["BloodPressure"].describe())

print()

print(df[df["Outcome"] == 1]["BloodPressure"].describe())

print()

print(df["BloodPressure"].describe())

print()



df.loc[(df["Outcome"] == 1) & (df["BMI"] == 0), "BMI"] = np.mean(df["BMI"][df["Outcome"] == 1])

df.loc[(df["Outcome"] == 0) & (df["BMI"] == 0), "BMI"] = np.mean(df["BMI"][df["Outcome"] == 0])

df.loc[(df["Outcome"] == 1) & (df["BloodPressure"] == 0), "BloodPressure"] = int(np.mean(df["BloodPressure"][df["Outcome"] == 1]))

df.loc[(df["Outcome"] == 0) & (df["BloodPressure"] == 0), "BloodPressure"] = int(np.mean(df["BloodPressure"][df["Outcome"] == 0]))

print(df[df["Outcome"] == 0]["BMI"].describe())

print()

print(df[df["Outcome"] == 1]["BMI"].describe())

print()

print(df["BMI"].describe())

print()

print(df[df["Outcome"] == 0]["BloodPressure"].describe())

print()

print(df[df["Outcome"] == 1]["BloodPressure"].describe())

print()

print(df["BloodPressure"].describe())

print()



print(df[df['Glucose'] == 0].shape[0])
print(df[df['Insulin'] == 0].shape[0])
print(df[df['BMI'] == 0].shape[0])
print(df[df['BloodPressure'] == 0].shape[0])
print(df[df['SkinThickness'] == 0].shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BloodPressure"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0) & (df["BloodPressure"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Outcome"] == 1)]).shape[0])
print(pd.DataFrame(df[(df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0])


df.loc[(df["Outcome"] == 1) & (df["Glucose"] == 0), "Glucose"] = int(np.mean(df["Glucose"][df["Outcome"] == 1]))

df.loc[(df["Outcome"] == 0) & (df["Glucose"] == 0), "Glucose"] = int(np.mean(df["Glucose"][df["Outcome"] == 0]))

df.loc[(df["Outcome"] == 1) & (df["SkinThickness"] == 0), "SkinThickness"] = int(np.mean(df["SkinThickness"][df["Outcome"] == 1]))

df.loc[(df["Outcome"] == 0) & (df["SkinThickness"] == 0), "SkinThickness"] = int(np.mean(df["SkinThickness"][df["Outcome"] == 0]))

print(df[df['Glucose'] == 0].shape[0])
print(df[df['Insulin'] == 0].shape[0])
print(df[df['BMI'] == 0].shape[0])
print(df[df['BloodPressure'] == 0].shape[0])
print(df[df['SkinThickness'] == 0].shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BloodPressure"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0) & (df["BloodPressure"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Outcome"] == 1)]).shape[0])
print(pd.DataFrame(df[(df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0])


df.hist(bins = 30, figsize = (10, 10), color = 'orange')

plt.show()


# Check the number of records left after cleaning
df.shape


# Create a correlation matrix

corr_matrix = df.corr()

corr_matrix


# Plot the correlation matrix
sns.set_context('talk')

plt.figure(figsize = (10, 10))

_ = sns.heatmap(corr_matrix, annot = True, fmt = ".3f", linewidths = .5)

plt.show()

fig = _.get_figure()

fig.savefig('img/corr_mat_diabetes.png')


## Export the histograms to an image file
# Determine the number of rows and columns for subplots
num_cols = 3
num_rows = -(-len(df.columns) // num_cols)  # Ceiling division to ensure enough rows

# Create a figure and axes
fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 3*num_rows))

# Flatten the axes array for easy iteration
axes = axes.flatten()

# Plot histogram for each column
for i, (col, ax) in enumerate(zip(df.columns, axes)):
    df[col].plot(kind='hist', bins=30, ax=ax, color='orange')
    ax.set_title(col)

# Remove any extra empty subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

# Adjust layout
plt.tight_layout()

# Save the figure
plt.savefig('img/histograms_diabetes.png')

# Show the plot
plt.show()











# split the dataframe into target and features

y = df["Outcome"] # target
X = df.drop(columns = ["Outcome"]) # features

# Verify that the split was performed correctly
print(X.shape)
print(y.shape)


from collections import Counter

counter = Counter(y)
print(counter)


# estimate scale_pos_weight value
estimate = counter[0] / counter[1]
print('Estimate: %.3f' % estimate)


# split the labels and features into training and testing sets

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 21, stratify = y)

# Verify that the split was performed correctly
print('Training set')
print(X_train.shape)
print(y_train.shape)
print()
print('Testing set')
print(X_test.shape)
print(y_test.shape)
print()


# Verify that the index has been shuffled
print(X.index)
print()
print(X_train.index)


from sklearn.preprocessing import RobustScaler

# Create a StandardScaler object
scaler = RobustScaler()

# Fit and transform the training data
X_train_scaled = scaler.fit_transform(X_train)

# Transform the test data (using the same scaling parameters as the training data)
X_test_scaled = scaler.transform(X_test)

display(pd.DataFrame(X_train_scaled).head())

print()

display(X_train.head())





# import the classifier

from xgboost import XGBClassifier


xgb_classifier = XGBClassifier(objective = 'binary:logistic', 
                               eval_metric = 'error', 
                               learning_rate = 0.1,
                               max_depth = 8,
                               alpha = 25,
                               n_estimators = 100,
                               scale_pos_weight=1.908
                               )

xgb_classifier.fit(X_train, y_train)





# predict the performance score of the trained model using the testing dataset

result = xgb_classifier.score(X_test, y_test)
print("Accuracy: {}".format(result))


# make predictions on the test data

y_predict = xgb_classifier.predict(X_test)
y_predict


# print the performance report

from sklearn.metrics import classification_report

print(classification_report(y_test, y_predict))


# print the confusion matrix

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_predict)

sns.set_context('talk')

sns.heatmap(cm, fmt = 'd', annot = True, cmap = 'RdGy')

plt.savefig('img/conf_mat_diabetes.png')






xgb_classifier.fit(X_train_scaled, y_train)

# predict the performance score of the trained model using the testing dataset

result = xgb_classifier.score(X_test_scaled, y_test)
print("Accuracy: {}".format(result))

# make predictions on the test data
y_predict = xgb_classifier.predict(X_test_scaled)

# print the performance report

from sklearn.metrics import classification_report

print(classification_report(y_test, y_predict))

# print the confusion matrix

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_predict)

sns.set_context('talk')

sns.heatmap(cm, fmt = 'd', annot = True, cmap = 'RdGy')

plt.savefig('img/conf_mat_diabetes.png')


from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.metrics import accuracy_score


X_train = X_train.drop(columns = "DiabetesPedigreeFunction") 
X_test = X_test.drop(columns = "DiabetesPedigreeFunction") 


selected_features = SelectKBest(chi2, k = 6).fit(X_train, y_train)

print('Score List: ', selected_features.scores_)
print()
print('Feature list: ', X_train.columns)


X_train_2 = selected_features.transform(X_train)

X_test_2 = selected_features.transform(X_test)

evalset = [(X_train_2, y_train), (X_test_2, y_test)]

xgb_classifier_2 = XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'logloss', 
                                 learning_rate = 0.02,
                                 max_depth = 8,
                                 alpha = 17,
                                 n_estimators = 230,                
                                 min_child_weight = 1,                                 
                                 scale_pos_weight = 1.908,
                                 use_label_encoder = False,
                                 seed = 21).fit(X_train_2, y_train, eval_set = evalset, verbose = 0)

result2 = xgb_classifier_2.score(X_test_2, y_test)
print()
print("Accuracy: {}".format(result2))

print()
print('Accuracy is: ', accuracy_score(y_test, xgb_classifier_2.predict(X_test_2)))
print()

display(pd.DataFrame(X_train_2).head())
display(X_train.head())

cm_2 = confusion_matrix(y_test, xgb_classifier_2.predict(X_test_2))

sns.heatmap(cm_2, annot = True, fmt = 'd')


# stratified k-fold cross validation evaluation of xgboost model
import xgboost
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score

# CV model
model = xgboost.XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'auc', 
                                 learning_rate = 0.0045,
                                 max_depth = 10,
                                 alpha = 17,
                                 n_estimators = 200,                
                                 min_child_weight = 1,                                 
                                 scale_pos_weight = 1.853,
                                 use_label_encoder = False,
                                 seed = 21)
kfold = StratifiedKFold(n_splits=15, shuffle = True, random_state=21)
results = cross_val_score(model, X_train, y_train, cv=kfold)
print("Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))


xgb_classifier_3 = XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'auc', 
                                 learning_rate = 0.0045,
                                 max_depth = 10,
                                 alpha = 17,
                                 n_estimators = 200,                
                                 min_child_weight = 1,                                 
                                 scale_pos_weight = 1.853,
                                 use_label_encoder = False,
                                 seed = 21
                               )

xgb_classifier_3.fit(X_train, y_train)

# predict the performance score of the trained model using the testing dataset

result3 = xgb_classifier_3.score(X_test, y_test)

y_pred_3 = xgb_classifier_3.predict(X_test)


cm_3 = confusion_matrix(y_test, y_pred_3)

sns.heatmap(cm_3, annot = True, fmt = 'd')

print()
print(classification_report(y_test, y_pred_3))

print()
print("Accuracy: {}".format(round(result3, 4)))

print()
# Calculate log loss
log_loss_3 = log_loss(y_test, xgb_classifier_3.predict_proba(X_test))

print("Log Loss: {}".format(round(log_loss_3, 4)))

print()

# Calculate ROC AUC
roc_auc_3 = roc_auc_score(y_test, xgb_classifier_3.predict_proba(X_test)[:, 1])

print("ROC AUC: {}".format(round(roc_auc_3, 4)))

print()


df_less_features = df.drop(columns = ["Pregnancies", "DiabetesPedigreeFunction"])

# split data into X and y
y_less = df_less_features["Outcome"] # target
X_less = df_less_features.drop(columns = ["Outcome"]) # features

X_less_train, X_less_test, y_less_train, y_less_test = train_test_split(X_less, y_less, test_size = 0.23, random_state = 21, stratify = y)

selected_features = SelectKBest(chi2, k = 5).fit(X_less_train, y_less_train)

print()
print('Score List: ', selected_features.scores_)
print()
print('Feature list: ', X_train.columns)
print()
X_train_4 = selected_features.transform(X_less_train)

X_test_4 = selected_features.transform(X_less_test)

#Compare the transformed and original array
print("Reduced features array")

display(pd.DataFrame(X_train_4).head())

print()
print("Original training set")

display(pd.DataFrame(X_less_train).head())






#Return the transformed arrays to a dataframe with the corresponding column names to keep consistent with the test set
X_train_4 = pd.DataFrame(X_train_4, columns = ["Glucose", "SkinThickness", "Insulin", "BMI", "Age"])
X_test_4 = pd.DataFrame(X_test_4, columns = ["Glucose", "SkinThickness", "Insulin", "BMI", "Age"])

evalset = [(X_train_4, y_less_train), (X_test_4, y_less_test)]


xgb_classifier_4 = XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'auc', 
                                 learning_rate = 0.0045,
                                 max_depth = 10,
                                 alpha = 17,
                                 n_estimators = 200,                
                                 min_child_weight = 1,                                 
                                 scale_pos_weight = 1.853,
                                 use_label_encoder = False,
                                 seed = 21).fit(X_train_4, y_less_train, verbose = 0)

result4 = xgb_classifier_4.score(X_test_4, y_less_test)

y_pred_4 = xgb_classifier_4.predict(X_test_4) 

y_pred_proba_4 = xgb_classifier_4.predict_proba(X_test_4)


# cm_4 = confusion_matrix(y_less_test, y_pred_4)

# sns.heatmap(cm_4, annot = True, fmt = 'd')

print()
print(classification_report(y_less_test, y_pred_4, digits = 4))

print()
print("Accuracy: {}".format(round(result4, 4)))

print()
# Calculate log loss
log_loss_4 = log_loss(y_less_test, y_pred_proba_4)

print("Log Loss: {}".format(round(log_loss_4, 4)))

print()

# Calculate ROC AUC
roc_auc_4 = roc_auc_score(y_less_test, y_pred_proba_4[:, 1])

print("ROC AUC: {}".format(round(roc_auc_4, 4)))

print()

# Calculate Precision-Recall curve
precision, recall, thresholds = precision_recall_curve(y_less_test, y_pred_proba_4[:, 1])

# Define recall levels for interpolation
recall_levels = np.linspace(0, 1, 100)

# Interpolate precision values for each recall level
interp_precision = np.interp(recall_levels, recall[::-1], precision[::-1])

# Calculate AUPRC using the trapezoidal rule
auprc = np.trapz(interp_precision, recall_levels)

print("AUPRC: {}".format(round(auprc, 4)))

print()


from sklearn.model_selection import StratifiedKFold

# Create a StratifiedKFold object with 3 splits
skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

X_train_4 = pd.DataFrame(X_train_4)

def strat_objective(trial):
    # Define the hyperparameter search space
    max_depth = trial.suggest_int('max_depth', 2, 15)
    n_estimators = trial.suggest_int('n_estimators', 100, 500)
    learning_rate = trial.suggest_float('learning_rate', 1e-3, 5e-1, log=True)
    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)
    gamma = trial.suggest_float('gamma', 1e-8, 1.0, log=True)
    subsample = trial.suggest_float('subsample', 0.5, 1.0)
    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)
    reg_alpha = trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True)
    reg_lambda = trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True)
    scale_pos_weight = 1.853  # Use the value you calculated
    # Create the XGBClassifier with the sampled hyperparameters
    # Create the XGBClassifier with the sampled hyperparameters
    classifier = XGBClassifier(
        max_depth=max_depth,
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        min_child_weight=min_child_weight,
        gamma=gamma,
        subsample=subsample,
        colsample_bytree=colsample_bytree,
        reg_alpha=reg_alpha,
        reg_lambda=reg_lambda,
        scale_pos_weight=scale_pos_weight,
        objective='binary:logistic',
        eval_metric='auc',
        use_label_encoder=False,
        seed=21,
        n_jobs=-1,
    )
    
    # Perform stratified cross-validation and return the negative mean score
    scores = []
    for train_index, val_index in skf.split(X_train_4, y_less_train):
        X_train_strat, X_val = X_train_4.iloc[train_index], X_train_4.iloc[val_index]
        y_train_strat, y_val = y_less_train.iloc[train_index], y_less_train.iloc[val_index]
        classifier.fit(X_train_strat, y_train_strat)
        score = roc_auc_score(y_val, classifier.predict_proba(X_val)[:, 1])
        scores.append(score)

    return -np.mean(scores)

# Create an Optuna study
study = optuna.create_study(direction='minimize')

# Optimize the objective function
optuna.logging.set_verbosity(optuna.logging.WARNING)
study.optimize(strat_objective, n_trials=100)

# Print the best hyperparameters and the corresponding score
print()
print('Best hyperparameters: ', study.best_params)
print()
print('Best score: ', -study.best_value)
print()


xgb_classifier_opt = XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'auc', 
                                 learning_rate = 0.01430548844650642,
                                 max_depth = 3,
                                 #alpha = 17,
                                 gamma = 0.2065076007167739,
                                 n_estimators = 317,                
                                 min_child_weight = 5,
                                 subsample = 0.8415801374501213,
                                 colsample_bytree = 0.5518694039036027,
                                 reg_alpha = 3.7522765281258646e-08,
                                 reg_lambda = 0.0003508483191270577,
                                 scale_pos_weight = 1.853,
                                 use_label_encoder = False,
                                 seed = 21).fit(X_train_4, y_less_train, verbose = 0)

result_opt = xgb_classifier_opt.score(X_test_4, y_less_test)

y_pred_opt = xgb_classifier_opt.predict(X_test_4) 

y_pred_proba_opt = xgb_classifier_opt.predict_proba(X_test_4)



print()
print(classification_report(y_less_test, y_pred_opt))

print()
print("Accuracy: {}".format(round(result_opt, 4)))

print()
# Calculate log loss
log_loss_opt = log_loss(y_less_test, y_pred_proba_opt)

print("Log Loss: {}".format(round(log_loss_opt, 4)))

print()

# Calculate ROC AUC
roc_auc_opt = roc_auc_score(y_less_test, y_pred_proba_opt[:, 1])

print("ROC AUC: {}".format(round(roc_auc_opt, 4)))

print()

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_less_test, y_pred_proba_opt[:, 1])

# Calculate Precision-Recall curve
precision, recall, thresholds = precision_recall_curve(y_less_test, y_pred_proba_opt[:, 1])

# Define recall levels for interpolation
recall_levels = np.linspace(0, 1, 100)

# Interpolate precision values for each recall level
interp_precision_opt = np.interp(recall_levels, recall[::-1], precision[::-1])

# Calculate AUPRC using the trapezoidal rule
auprc_opt = np.trapz(interp_precision_opt, recall_levels)

print("AUPRC: {}".format(round(auprc_opt, 4)))

print()




from imblearn.over_sampling import SMOTE

smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X_train_4, y_less_train)

xgb_classifier_resampled = XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'auc', 
                                 learning_rate = 0.01430548844650642,
                                 max_depth = 3,
                                 #alpha = 17,
                                 gamma = 0.2065076007167739,
                                 n_estimators = 317,                
                                 min_child_weight = 5,
                                 subsample = 0.8415801374501213,
                                 colsample_bytree = 0.5518694039036027,
                                 reg_alpha = 3.7522765281258646e-08,
                                 reg_lambda = 0.0003508483191270577,
                                 #scale_pos_weight = 1.853,
                                 use_label_encoder = False,
                                 seed = 21).fit(X_resampled, y_resampled, verbose = 0)

result_resampled = xgb_classifier_resampled.score(X_test_4, y_less_test)

y_pred_resampled = xgb_classifier_resampled.predict(X_test_4) 

y_pred_proba_resampled = xgb_classifier_resampled.predict_proba(X_test_4)

print()
print(classification_report(y_less_test, y_pred_resampled))

print()
print("Accuracy: {}".format(round(result_resampled, 4)))

print()
# Calculate log loss
log_loss_resampled = log_loss(y_less_test, y_pred_proba_resampled)

print("Log Loss: {}".format(round(log_loss_resampled, 4)))

print()

# Calculate ROC AUC
roc_auc_resampled = roc_auc_score(y_less_test, y_pred_proba_resampled[:, 1])

print("ROC AUC: {}".format(round(roc_auc_resampled, 4)))

print()

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_less_test, y_pred_proba_resampled[:, 1])

# Calculate Precision-Recall curve
precision, recall, thresholds = precision_recall_curve(y_less_test, y_pred_proba_resampled[:, 1])

# Define recall levels for interpolation
recall_levels = np.linspace(0, 1, 100)

# Interpolate precision values for each recall level
interp_precision = np.interp(recall_levels, recall[::-1], precision[::-1])

# Calculate AUPRC using the trapezoidal rule
auprc = np.trapz(interp_precision, recall_levels)

print("AUPRC: {}".format(round(auprc, 4)))




import optuna
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score

# Define the objective function to optimize
def objective(trial):
    # Define the hyperparameter search space
    max_depth = trial.suggest_int('max_depth', 2, 15)
    n_estimators = trial.suggest_int('n_estimators', 100, 500)
    learning_rate = trial.suggest_float('learning_rate', 1e-3, 5e-1, log=True)
    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)
    gamma = trial.suggest_float('gamma', 1e-8, 1.0, log=True)
    subsample = trial.suggest_float('subsample', 0.5, 1.0)
    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)
    reg_alpha = trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True)
    reg_lambda = trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True)
    #scale_pos_weight = 1.853  # Use the value you calculated

    # Create the XGBClassifier with the sampled hyperparameters
    classifier = XGBClassifier(
        max_depth=max_depth,
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        min_child_weight=min_child_weight,
        gamma=gamma,
        subsample=subsample,
        colsample_bytree=colsample_bytree,
        reg_alpha=reg_alpha,
        reg_lambda=reg_lambda,
        #scale_pos_weight=scale_pos_weight,
        objective='binary:logistic',
        eval_metric='auc',
        use_label_encoder=False,
        seed=21,
        n_jobs=-1,
    )

    # Perform cross-validation and return the negative mean score
    score = cross_val_score(classifier, X_resampled, y_resampled, cv=5, scoring='roc_auc', n_jobs=-1)
    return -score.mean()

# Create an Optuna study
study = optuna.create_study(direction='minimize')

# Optimize the objective function
optuna.logging.set_verbosity(optuna.logging.WARNING)
study.optimize(objective, n_trials=100)

# Print the best hyperparameters and the corresponding score
print()
print('Best hyperparameters: ', study.best_params)
print()
print('Best score: ', -study.best_value)
print()


xgb_classifier_resampled = XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'auc', 
                                 learning_rate = 0.014706610661608144,
                                 max_depth = 3,
                                 #alpha = 17,
                                 gamma = 0.2065076007167739,
                                 n_estimators = 317,                
                                 min_child_weight = 4,
                                 subsample = 0.8415801374501213,
                                 colsample_bytree = 0.5587081098136418,
                                 reg_alpha = 3.7522765281258646e-02,
                                 reg_lambda = 0.0003085699198955287,
                                 #scale_pos_weight = 1.866,
                                 use_label_encoder = False,
                                 seed = 21).fit(X_resampled, y_resampled, verbose = 0)

result_resampled = xgb_classifier_resampled.score(X_test_4, y_less_test)

y_pred_resampled = xgb_classifier_resampled.predict(X_test_4) 

y_pred_proba_resampled = xgb_classifier_resampled.predict_proba(X_test_4)


print()
print(classification_report(y_less_test, y_pred_resampled))

print()
print("Accuracy: {}".format(round(result_resampled, 4)))

print()
# Calculate log loss
log_loss_resampled = log_loss(y_less_test, y_pred_proba_resampled)

print("Log Loss: {}".format(round(log_loss_resampled, 4)))

print()

# Calculate ROC AUC
roc_auc_resampled = roc_auc_score(y_less_test, y_pred_proba_resampled[:, 1])

print("ROC AUC: {}".format(round(roc_auc_resampled, 4)))

print()

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_less_test, y_pred_proba_resampled[:, 1])

# Calculate Precision-Recall curve
precision, recall, thresholds = precision_recall_curve(y_less_test, y_pred_proba_resampled[:, 1])

# Define recall levels for interpolation
recall_levels = np.linspace(0, 1, 100)

# Interpolate precision values for each recall level
interp_precision = np.interp(recall_levels, recall[::-1], precision[::-1])

# Calculate AUPRC using the trapezoidal rule
auprc = np.trapz(interp_precision, recall_levels)

print("AUPRC: {}".format(round(auprc, 4)))

print()




from hyperopt import hp, fmin, tpe, STATUS_OK, Trials
from sklearn.model_selection import cross_val_score

# Define the hyperparameter search space
space = {
    'max_depth': hp.quniform('max_depth', 2, 15, 1),
    'n_estimators': hp.quniform('n_estimators', 100, 500, 1),
    'learning_rate': hp.loguniform('learning_rate', np.log(1e-3), np.log(5e-1)),
    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),
    'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),
    'subsample': hp.uniform('subsample', 0.5, 1.0),
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),
    'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(1.0)),
    'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-8), np.log(1.0)),
}

# Define the objective function to optimize
def objective(params):
    classifier = XGBClassifier(
        max_depth=int(params['max_depth']),
        n_estimators=int(params['n_estimators']),
        learning_rate=params['learning_rate'],
        min_child_weight=int(params['min_child_weight']),
        gamma=params['gamma'],
        subsample=params['subsample'],
        colsample_bytree=params['colsample_bytree'],
        reg_alpha=params['reg_alpha'],
        reg_lambda=params['reg_lambda'],
        objective='binary:logistic',
        eval_metric='aucpr',
        use_label_encoder=False,
        seed=21,
        n_jobs=-1,
    )

    score = cross_val_score(classifier, X_train_4, y_less_train, cv=5, scoring='roc_auc', n_jobs=-1)
    return {'loss': -score.mean(), 'status': STATUS_OK}

# Create a Trials object to track the optimization process
trials = Trials()

# Optimize the objective function using fmin
best = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,
    max_evals=100,
    trials=trials,
    rstate=np.random.default_rng(21)
)

# Print the best hyperparameters and the corresponding score
print("Best hyperparameters:", best)


xgb_classifier_hyp = XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'aucpr', 
                                 learning_rate = 0.021446780130612256,
                                 max_depth = 2,
                                 alpha = 17,
                                 gamma = 2.0973587186681374e-07,
                                 n_estimators = 278,                
                                 min_child_weight = 1,
                                 subsample = 0.8679199231900535,
                                 colsample_bytree = 0.5568212105951751,
                                 reg_alpha = 3.965872220695349e-07,
                                 reg_lambda = 2.586145177494886e-07,
                                 scale_pos_weight = 1.853,
                                 use_label_encoder = False,
                                 seed = 21).fit(X_train_4, y_less_train, verbose = 0)

result_hyp = xgb_classifier_hyp.score(X_test_4, y_less_test)

y_pred_hyp = xgb_classifier_hyp.predict(X_test_4) 

y_pred_proba_hyp = xgb_classifier_hyp.predict_proba(X_test_4)


print()
print(classification_report(y_less_test, y_pred_hyp, digits = 4))

print()
print("Accuracy: {}".format(round(result_hyp, 4)))

print()
# Calculate log loss
log_loss_hyp = log_loss(y_less_test, y_pred_proba_hyp)

print("Log Loss: {}".format(round(log_loss_hyp, 4)))

print()

# Calculate ROC AUC
roc_auc_hyp = roc_auc_score(y_less_test, y_pred_proba_hyp[:, 1])

print("ROC AUC: {}".format(round(roc_auc_hyp, 4)))

print()

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_less_test, y_pred_proba_hyp[:, 1])

# Calculate Precision-Recall curve
precision, recall, thresholds = precision_recall_curve(y_less_test, y_pred_proba_hyp[:, 1])

# Define recall levels for interpolation
recall_levels = np.linspace(0, 1, 100)

# Interpolate precision values for each recall level
interp_precision = np.interp(recall_levels, recall[::-1], precision[::-1])

# Calculate AUPRC using the trapezoidal rule
auprc = np.trapz(interp_precision, recall_levels)

print("AUPRC: {}".format(round(auprc, 4)))

print()




import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import average_precision_score

# LightGBM with Stratified K-Fold Cross-Validation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=21)

# Define the objective function for LightGBM
def lgb_objective(trial):
    params = {
        'boosting_type': 'gbdt',
        'objective': 'binary',
        'metric': 'auc',
        'max_depth': trial.suggest_int('max_depth', 2, 15),
        'num_leaves': trial.suggest_int('num_leaves', 10, 100),
        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 5e-1, log=True),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'random_state': 21,
        'n_jobs': -1, 
        'verbosity': -1,
    }

    scores = cross_val_score(lgb.LGBMClassifier(**params), X_train_4, y_less_train, cv=skf, scoring='roc_auc', n_jobs=-1)
    return -scores.mean()

# Create an Optuna study
study = optuna.create_study(direction='minimize')

# Optimize the objective function
optuna.logging.set_verbosity(optuna.logging.WARNING)
study.optimize(lgb_objective, n_trials=100)

# Print the best hyperparameters and the corresponding score
print()
print("Best hyperparameters:", study.best_params)
print()
print("Best score:", -study.best_value)
print()



# Train the LightGBM model with the best hyperparameters
best_params = study.best_params
lgb_model = lgb.LGBMClassifier(**best_params, verbosity = -1)
lgb_model.fit(X_train_4, y_less_train)

# Evaluate the model on the test set
result_lgb = lgb_model.score(X_test_4, y_less_test)
y_pred_lgb = lgb_model.predict(X_test_4) 
y_pred_proba_lgb = lgb_model.predict_proba(X_test_4)[:, 1]
roc_auc_lgb = roc_auc_score(y_less_test, y_pred_proba_lgb)
auprc_lgb = average_precision_score(y_less_test, y_pred_proba_lgb)

print()
print(classification_report(y_less_test, y_pred_lgb, digits = 4))

print()
print("Accuracy: {}".format(round(result_lgb, 4)))

# Calculate log loss
log_loss_lgb = log_loss(y_less_test, y_pred_proba_lgb)
print()
print("Log Loss: {}".format(round(log_loss_lgb, 4)))
print()
print("ROC AUC (LightGBM):", round(roc_auc_lgb, 4))
print()
print("AUPRC (LightGBM):", round(auprc_lgb, 4))


from catboost import CatBoostClassifier
from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score

# Consider StratifiedShuffleSplit for imbalanced classes
skf = StratifiedShuffleSplit(n_splits=5, random_state=21)

# Define the objective function with early stopping
def catboost_objective(trial):
    params = {
        'objective': 'Logloss',
        'eval_metric': 'AUC',
        'max_depth': trial.suggest_int('max_depth', 2, 15),
        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 5e-1, log=True),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 1.0, log=True),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'random_strength': trial.suggest_int('random_strength', 0, 100),
        'random_state': 21,
        'thread_count': -1,  # Enable GPU if available
    }

    best_score = float('inf')  # Initialize best score as positive infinity
    early_stopping_rounds = 5  # Number of rounds for early stopping

    for train_index, val_index in skf.split(X_train_4, y_less_train):
        X_train_strat, X_val = X_train_4.iloc[train_index], X_train_4.iloc[val_index]
        y_train_strat, y_val = y_less_train.iloc[train_index], y_less_train.iloc[val_index]

        model = CatBoostClassifier(**params, verbose=False)
        model.fit(X_train_strat, y_train_strat, eval_set=(X_val, y_val), early_stopping_rounds=early_stopping_rounds)

        score = model.get_best_score()['validation']['AUC']  # Assuming AUC is your metric
        if score > best_score:
            best_score = score
            continue  # Continue training if score improves

        # Early stopping if score doesn't improve for 'early_stopping_rounds'
        break

    return -best_score

# Create an Optuna study
study = create_study(direction='minimize')

# Optimize the objective function
optuna.logging.set_verbosity(optuna.logging.WARNING)
study.optimize(catboost_objective, n_trials=150)

# Print the best hyperparameters and the corresponding score
print()
print("Best hyperparameters:", study.best_params)
print()
print("Best score:", -study.best_value)
print()



# Train the CatBoost model with the best hyperparameters
best_params = study.best_params
catboost_model = CatBoostClassifier(**best_params, verbose=False)
catboost_model.fit(X_train_4, y_less_train)

# Evaluate the model on the test set
y_pred_proba_catboost = catboost_model.predict_proba(X_test_4)[:, 1]
roc_auc_catboost = roc_auc_score(y_less_test, y_pred_proba_catboost)
auprc_catboost = average_precision_score(y_less_test, y_pred_proba_catboost)

print("ROC AUC (CatBoost):", roc_auc_catboost)
print("AUPRC (CatBoost):", auprc_catboost)



