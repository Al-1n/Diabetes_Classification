#!pip install optuna
#!pip install jupyterlab-optuna
#!pip install optuna-fast-fanova gunicorn


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import log_loss, roc_auc_score



df = pd.read_csv("./Data/diabetes.csv", sep = ",")

df.head()





#Verify that all columns have numeric types
df.dtypes


#Check the target variable label values
df["Outcome"].value_counts()


#Get the summary sttistics for all the columns
df.describe()


df.hist(bins = 30, figsize = (20, 20), color = 'orange')

plt.show()





print(df[df['Glucose'] == 0].shape[0])
print(df[df['Insulin'] == 0].shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BloodPressure"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0) & (df["BloodPressure"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Outcome"] == 1)]).shape[0])
print(pd.DataFrame(df[(df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0])





print(df[df["Outcome"] == 0]["Insulin"].describe())

print()

print(df[df["Outcome"] == 1]["Insulin"].describe())

print()

print(df["Insulin"].describe())

print()

print(np.median(df[(df["Outcome"] == 1) & (df["Insulin"] != 0)]["Insulin"]))


df["Insulin"].hist(bins = 30)


import numpy as np

df.loc[(df["Outcome"] == 1) & (df["Insulin"] == 0), "Insulin"] = int(np.mean(df["Insulin"]))

df.loc[(df["Outcome"] == 0) & (df["Insulin"] == 0), "Insulin"] = int(np.median(df["Insulin"]))


print(df[df["Outcome"] == 0]["Insulin"].describe())

print()

print(df[df["Outcome"] == 1]["Insulin"].describe())

print()

print(df["Insulin"].describe())

print()

print(np.median(df[df["Outcome"] == 1]["Insulin"]))




df["Insulin"].hist(bins = 30)


print(df[df['Glucose'] == 0].shape[0])
print(df[df['Insulin'] == 0].shape[0])
print(df[df['BMI'] == 0].shape[0])
print(df[df['BloodPressure'] == 0].shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BloodPressure"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0) & (df["BloodPressure"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Outcome"] == 1)]).shape[0])
print(pd.DataFrame(df[(df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0])


print(df[df["Outcome"] == 0]["BMI"].describe())

print()

print(df[df["Outcome"] == 1]["BMI"].describe())

print()

print(df["BMI"].describe())

print()

print(df[df["Outcome"] == 0]["BloodPressure"].describe())

print()

print(df[df["Outcome"] == 1]["BloodPressure"].describe())

print()

print(df["BloodPressure"].describe())

print()



df.loc[(df["Outcome"] == 1) & (df["BMI"] == 0), "BMI"] = np.mean(df["BMI"][df["Outcome"] == 1])

df.loc[(df["Outcome"] == 0) & (df["BMI"] == 0), "BMI"] = np.mean(df["BMI"][df["Outcome"] == 0])

df.loc[(df["Outcome"] == 1) & (df["BloodPressure"] == 0), "BloodPressure"] = int(np.mean(df["BloodPressure"][df["Outcome"] == 1]))

df.loc[(df["Outcome"] == 0) & (df["BloodPressure"] == 0), "BloodPressure"] = int(np.mean(df["BloodPressure"][df["Outcome"] == 0]))

print(df[df["Outcome"] == 0]["BMI"].describe())

print()

print(df[df["Outcome"] == 1]["BMI"].describe())

print()

print(df["BMI"].describe())

print()

print(df[df["Outcome"] == 0]["BloodPressure"].describe())

print()

print(df[df["Outcome"] == 1]["BloodPressure"].describe())

print()

print(df["BloodPressure"].describe())

print()



print(df[df['Glucose'] == 0].shape[0])
print(df[df['Insulin'] == 0].shape[0])
print(df[df['BMI'] == 0].shape[0])
print(df[df['BloodPressure'] == 0].shape[0])
print(df[df['SkinThickness'] == 0].shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BloodPressure"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0) & (df["BloodPressure"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Outcome"] == 1)]).shape[0])
print(pd.DataFrame(df[(df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0])


df.loc[(df["Outcome"] == 1) & (df["Glucose"] == 0), "Glucose"] = int(np.mean(df["Glucose"][df["Outcome"] == 1]))

df.loc[(df["Outcome"] == 0) & (df["Glucose"] == 0), "Glucose"] = int(np.mean(df["Glucose"][df["Outcome"] == 0]))

df.loc[(df["Outcome"] == 1) & (df["SkinThickness"] == 0), "SkinThickness"] = int(np.mean(df["SkinThickness"][df["Outcome"] == 1]))

df.loc[(df["Outcome"] == 0) & (df["SkinThickness"] == 0), "SkinThickness"] = int(np.mean(df["SkinThickness"][df["Outcome"] == 0]))

print(df[df['Glucose'] == 0].shape[0])
print(df[df['Insulin'] == 0].shape[0])
print(df[df['BMI'] == 0].shape[0])
print(df[df['BloodPressure'] == 0].shape[0])
print(df[df['SkinThickness'] == 0].shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BloodPressure"] == 0) & (df["Insulin"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0) & (df["BloodPressure"] == 0)]).shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0])
print(pd.DataFrame(df[(df["Glucose"] == 0) & (df["Outcome"] == 1)]).shape[0])
print(pd.DataFrame(df[(df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0])


df.hist(bins = 30, figsize = (20, 20), color = 'orange')

plt.show()


# Check the number of records left after cleaning
df.shape


## Export the histograms to an image file
# Determine the number of rows and columns for subplots
num_cols = 3
num_rows = -(-len(df.columns) // num_cols)  # Ceiling division to ensure enough rows

# Create a figure and axes
fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, 5*num_rows))

# Flatten the axes array for easy iteration
axes = axes.flatten()

# Plot histogram for each column
for i, (col, ax) in enumerate(zip(df.columns, axes)):
    df[col].plot(kind='hist', bins=30, ax=ax, color='orange')
    ax.set_title(col)

# Remove any extra empty subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

# Adjust layout
plt.tight_layout()

# Save the figure
plt.savefig('img/histograms_diabetes.png')

# Show the plot
plt.show()


# Create a correlation matrix

corr_matrix = df.corr()

corr_matrix





# Plot the correlation matrix
sns.set_context('talk')

plt.figure(figsize = (16, 16))

_ = sns.heatmap(corr_matrix, annot = True, fmt = ".3f", linewidths = .5)

plt.show()

fig = _.get_figure()

fig.savefig('img/corr_mat_diabetes.png')








# split the dataframe into target and features

y = df["Outcome"] # target
X = df.drop(columns = ["Outcome"]) # features

# Verify that the split was performed correctly
print(X.shape)
print(y.shape)


from collections import Counter

counter = Counter(y)
print(counter)


# estimate scale_pos_weight value
estimate = counter[0] / counter[1]
print('Estimate: %.3f' % estimate)


# split the labels and features into training and testing sets

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 21, stratify = y)

# Verify that the split was performed correctly
print('Training set')
print(X_train.shape)
print(y_train.shape)
print()
print('Testing set')
print(X_test.shape)
print(y_test.shape)
print()


# Verify that the index has been shuffled
print(X.index)
print()
print(X_train.index)


from sklearn.preprocessing import RobustScaler

# Create a StandardScaler object
scaler = RobustScaler()

# Fit and transform the training data
X_train_scaled = scaler.fit_transform(X_train)

# Transform the test data (using the same scaling parameters as the training data)
X_test_scaled = scaler.transform(X_test)

display(pd.DataFrame(X_train_scaled).head())

print()

display(X_train.head())





# import the classifier

from xgboost import XGBClassifier


xgb_classifier = XGBClassifier(objective = 'binary:logistic', 
                               eval_metric = 'error', 
                               learning_rate = 0.1,
                               max_depth = 8,
                               alpha = 25,
                               n_estimators = 100,
                               scale_pos_weight=1.908
                               )

xgb_classifier.fit(X_train, y_train)





# predict the performance score of the trained model using the testing dataset

result = xgb_classifier.score(X_test, y_test)
print("Accuracy: {}".format(result))


# make predictions on the test data

y_predict = xgb_classifier.predict(X_test)
y_predict


# print the performance report

from sklearn.metrics import classification_report

print(classification_report(y_test, y_predict))


# print the confusion matrix

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_predict)

sns.set_context('talk')

sns.heatmap(cm, fmt = 'd', annot = True, cmap = 'RdGy')

plt.savefig('img/conf_mat_diabetes.png')






xgb_classifier.fit(X_train_scaled, y_train)

# predict the performance score of the trained model using the testing dataset

result = xgb_classifier.score(X_test_scaled, y_test)
print("Accuracy: {}".format(result))

# make predictions on the test data
y_predict = xgb_classifier.predict(X_test_scaled)

# print the performance report

from sklearn.metrics import classification_report

print(classification_report(y_test, y_predict))

# print the confusion matrix

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_predict)

sns.set_context('talk')

sns.heatmap(cm, fmt = 'd', annot = True, cmap = 'RdGy')

plt.savefig('img/conf_mat_diabetes.png')


from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.metrics import accuracy_score


X_train = X_train.drop(columns = "DiabetesPedigreeFunction") 
X_test = X_test.drop(columns = "DiabetesPedigreeFunction") 


selected_features = SelectKBest(chi2, k = 6).fit(X_train, y_train)

print('Score List: ', selected_features.scores_)
print()
print('Feature list: ', X_train.columns)


X_train_2 = selected_features.transform(X_train)

X_test_2 = selected_features.transform(X_test)

evalset = [(X_train_2, y_train), (X_test_2, y_test)]

xgb_classifier_2 = XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'logloss', 
                                 learning_rate = 0.02,
                                 max_depth = 8,
                                 alpha = 17,
                                 n_estimators = 230,                
                                 min_child_weight = 1,                                 
                                 scale_pos_weight = 1.908,
                                 use_label_encoder = False,
                                 seed = 21).fit(X_train_2, y_train, eval_set = evalset, verbose = 0)

result2 = xgb_classifier_2.score(X_test_2, y_test)
print()
print("Accuracy: {}".format(result2))

print()
print('Accuracy is: ', accuracy_score(y_test, xgb_classifier_2.predict(X_test_2)))
print()

display(pd.DataFrame(X_train_2).head())
display(X_train.head())

cm_2 = confusion_matrix(y_test, xgb_classifier_2.predict(X_test_2))

sns.heatmap(cm_2, annot = True, fmt = 'd')


results = xgb_classifier_2.evals_result()

plt.plot(results['validation_0']['logloss'], label = 'train')
plt.plot(results['validation_1']['logloss'], label = 'test')

plt.xlabel('Number of trees')
plt.ylabel('Error')

plt.legend()

plt.show()


print(result2)





# stratified k-fold cross validation evaluation of xgboost model
import xgboost
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score

# CV model
model = xgboost.XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'logloss', 
                                 learning_rate = 0.0045,
                                 max_depth = 10,
                                 alpha = 17,
                                 n_estimators = 200,                
                                 min_child_weight = 1,                                 
                                 scale_pos_weight = 1.853,
                                 use_label_encoder = False,
                                 seed = 21)
kfold = StratifiedKFold(n_splits=15, shuffle = True, random_state=21)
results = cross_val_score(model, X_train, y_train, cv=kfold)
print("Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))


xgb_classifier_3 = XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'logloss', 
                                 learning_rate = 0.0045,
                                 max_depth = 10,
                                 alpha = 17,
                                 n_estimators = 200,                
                                 min_child_weight = 1,                                 
                                 scale_pos_weight = 1.853,
                                 use_label_encoder = False,
                                 seed = 21
                               )

xgb_classifier_3.fit(X_train, y_train)

# predict the performance score of the trained model using the testing dataset

result3 = xgb_classifier_3.score(X_test, y_test)

y_pred_3 = xgb_classifier_3.predict(X_test)


cm_3 = confusion_matrix(y_test, y_pred_3)

sns.heatmap(cm_3, annot = True, fmt = 'd')

print()
print(classification_report(y_test, y_pred_3))

print()
print("Accuracy: {}".format(round(result3, 4)))

print()
# Calculate log loss
log_loss_3 = log_loss(y_test, xgb_classifier_3.predict_proba(X_test))

print("Log Loss: {}".format(round(log_loss_3, 4)))

print()

# Calculate ROC AUC
roc_auc_3 = roc_auc_score(y_test, xgb_classifier_3.predict_proba(X_test)[:, 1])

print("ROC AUC: {}".format(round(roc_auc_3, 4)))

print()


df_less_features = df.drop(columns = ["DiabetesPedigreeFunction", "Pregnancies", "BloodPressure"])

# split data into X and y
y_less = df_less_features["Outcome"] # target
X_less = df_less_features.drop(columns = ["Outcome"]) # features

X_less_train, X_less_test, y_less_train, y_less_test = train_test_split(X_less, y_less, test_size = 0.23, random_state = 21, stratify = y)

selected_features = SelectKBest(chi2, k = 4).fit(X_less_train, y_less_train)

print('Score List: ', selected_features.scores_)
print()
print('Feature list: ', X_train.columns)

X_train_4 = selected_features.transform(X_less_train)

X_test_4 = selected_features.transform(X_less_test)

evalset = [(X_train_4, y_less_train), (X_test_4, y_less_test)]

xgb_classifier_4 = XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'logloss', 
                                 learning_rate = 0.0045,
                                 max_depth = 10,
                                 alpha = 17,
                                 n_estimators = 200,                
                                 min_child_weight = 1,                                 
                                 scale_pos_weight = 1.853,
                                 use_label_encoder = False,
                                 seed = 21).fit(X_train_4, y_less_train, eval_set = evalset, verbose = 0)

result4 = xgb_classifier_4.score(X_test_4, y_less_test)

y_pred_4 = xgb_classifier_4.predict(X_test_4) 



cm_4 = confusion_matrix(y_less_test, y_pred_4)

sns.heatmap(cm_4, annot = True, fmt = 'd')

print()
print(classification_report(y_less_test, y_pred_4))

print()
print("Accuracy: {}".format(round(result4, 4)))

print()
# Calculate log loss
log_loss_4 = log_loss(y_less_test, xgb_classifier_4.predict_proba(X_test_4))

print("Log Loss: {}".format(round(log_loss_4, 4)))

print()

# Calculate ROC AUC
roc_auc_4 = roc_auc_score(y_less_test, xgb_classifier_4.predict_proba(X_test_4)[:, 1])

print("ROC AUC: {}".format(round(roc_auc_4, 4)))

print()


import optuna
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score

# Define the objective function to optimize
def objective(trial):
    # Define the hyperparameter search space
    max_depth = trial.suggest_int('max_depth', 2, 15)
    n_estimators = trial.suggest_int('n_estimators', 100, 500)
    learning_rate = trial.suggest_float('learning_rate', 1e-3, 5e-1, log=True)
    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)
    gamma = trial.suggest_float('gamma', 1e-8, 1.0, log=True)
    subsample = trial.suggest_float('subsample', 0.5, 1.0)
    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)
    reg_alpha = trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True)
    reg_lambda = trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True)
    scale_pos_weight = 1.853  # Use the value you calculated

    # Create the XGBClassifier with the sampled hyperparameters
    classifier = XGBClassifier(
        max_depth=max_depth,
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        min_child_weight=min_child_weight,
        gamma=gamma,
        subsample=subsample,
        colsample_bytree=colsample_bytree,
        reg_alpha=reg_alpha,
        reg_lambda=reg_lambda,
        scale_pos_weight=scale_pos_weight,
        objective='binary:logistic',
        eval_metric='logloss',
        use_label_encoder=False,
        seed=21,
        n_jobs=-1,
    )

    # Perform cross-validation and return the negative mean score
    score = cross_val_score(classifier, X_train_4, y_less_train, cv=5, scoring='roc_auc', n_jobs=-1)
    return -score.mean()

# Create an Optuna study
study = optuna.create_study(direction='minimize')

# Optimize the objective function
study.optimize(objective, n_trials=100)

# Print the best hyperparameters and the corresponding score
print('Best hyperparameters: ', study.best_params)
print('Best score: ', -study.best_value)


from sklearn.model_selection import StratifiedKFold

# Create a StratifiedKFold object with 3 splits
skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

X_train_4 = pd.DataFrame(X_train_4)

def strat_objective(trial):
    # Define the hyperparameter search space
    max_depth = trial.suggest_int('max_depth', 2, 15)
    n_estimators = trial.suggest_int('n_estimators', 100, 500)
    learning_rate = trial.suggest_float('learning_rate', 1e-3, 5e-1, log=True)
    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)
    gamma = trial.suggest_float('gamma', 1e-8, 1.0, log=True)
    subsample = trial.suggest_float('subsample', 0.5, 1.0)
    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)
    reg_alpha = trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True)
    reg_lambda = trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True)
    scale_pos_weight = 1.853  # Use the value you calculated
    # Create the XGBClassifier with the sampled hyperparameters
    # Create the XGBClassifier with the sampled hyperparameters
    classifier = XGBClassifier(
        max_depth=max_depth,
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        min_child_weight=min_child_weight,
        gamma=gamma,
        subsample=subsample,
        colsample_bytree=colsample_bytree,
        reg_alpha=reg_alpha,
        reg_lambda=reg_lambda,
        scale_pos_weight=scale_pos_weight,
        objective='binary:logistic',
        eval_metric='logloss',
        use_label_encoder=False,
        seed=21,
        n_jobs=-1,
    )
    
    # Perform stratified cross-validation and return the negative mean score
    scores = []
    for train_index, val_index in skf.split(X_train_4, y_less_train):
        X_train_strat, X_val = X_train_4.iloc[train_index], X_train_4.iloc[val_index]
        y_train_strat, y_val = y_less_train.iloc[train_index], y_less_train.iloc[val_index]
        classifier.fit(X_train_strat, y_train_strat)
        score = roc_auc_score(y_val, classifier.predict_proba(X_val)[:, 1])
        scores.append(score)

    return -np.mean(scores)

# Create an Optuna study
study = optuna.create_study(direction='minimize')

# Optimize the objective function
study.optimize(strat_objective, n_trials=100)

# Print the best hyperparameters and the corresponding score
print('Best hyperparameters: ', study.best_params)
print('Best score: ', -study.best_value)





