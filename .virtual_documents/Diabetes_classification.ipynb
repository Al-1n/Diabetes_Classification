#!pip install optuna
#!pip install jupyterlab-optuna
#!pip install optuna-fast-fanova gunicorn
#!pip install imbalanced-learn
#!pip install hyperopt
#!pip install lightgbm
#!pip install catboost
#!pip install optuna-integration


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import log_loss, roc_auc_score
from sklearn.metrics import roc_curve, precision_recall_curve
import matplotlib.pyplot as plt
import optuna
from IPython.display import Markdown, display

def printmd(string, color=None):
    colorstr = "<span style='color:{}'>{}</span>".format(color, string)
    display(Markdown(colorstr))


#load the dataset and preview the first rows
df = pd.read_csv("./Data/diabetes.csv", sep = ",")

df.head(10)





#Verify that all columns have numeric types
df.dtypes


#Check the target variable label values
df["Outcome"].value_counts()


#Get the summary sttistics for all the columns
df.describe()


df.hist(bins = 30, figsize = (10, 10), color = 'orange')

plt.show()





def print_missing_data_summary():
    def printmd(string, color=None):
        colorstr = "<span style='color:{}'>{}</span>".format(color, string)
        display(Markdown(colorstr))  
    
    v1 = pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0)]).shape[0]
    v2 = pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0)]).shape[0]
    v3 = pd.DataFrame(df[(df["BloodPressure"] == 0) & (df["Insulin"] == 0)]).shape[0]
    v4 = pd.DataFrame(df[(df["BMI"] == 0) & (df["Insulin"] == 0) & (df["BloodPressure"] == 0)]).shape[0]
    v5 = pd.DataFrame(df[(df["Glucose"] == 0) & (df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0]
    v6 = pd.DataFrame(df[(df["Glucose"] == 0) & (df["Outcome"] == 1)]).shape[0]
    v7 = pd.DataFrame(df[(df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0]

    display(Markdown('---'))
    printmd(f"**Number of missing glucose measurements**: {df[df['Glucose'] == 0].shape[0]}", color="#9DC183")
    printmd(f"**Number of missing insulin measurements**: {df[df['Insulin'] == 0].shape[0]}", color="#9DC183")
    printmd(f"**Number of missing BMI measurements**: {df[df['BMI'] == 0].shape[0]}", color="#9DC183")
    printmd(f"**Number of missing blood pressure measurements**: {df[df['BloodPressure'] == 0].shape[0]}", color="#9DC183")
    printmd(f"**Number of missing skin thickness measurements**: {df[df['SkinThickness'] == 0].shape[0]}", color="#9DC183")
    display(Markdown('---'))
    printmd(f"**Number of records missing both glucose and insulin measurements**: {v1}", color="#9DC183")
    printmd(f"**Number of records missing both BMI and Insulin data**: {v2}", color="#9DC183")
    printmd(f"**Number of records missing both Blood Pressure and Insulin data**: {v3}", color="#9DC183")
    printmd(f"**Number of records missing BMI, Insulin, and Blood Pressure measurements**: {v4}", color="#9DC183")
    printmd(f"**Number of records missing glucose and insulin measurements while the outcome is positive**: {v5}", color="#9DC183")
    printmd(f"**Number of missing glucose measurements while the outcome is positive**: {v6}", color="#9DC183")
    printmd(f"**Number of missing insulin measurements while the outcome is positive**: {v7}", color="#9DC183")

print_missing_data_summary()





from ipywidgets import widgets, Layout
from IPython import display as disp

def render_widgets(num_widgets, titles, objects):
    # Create output widgets
    widgets_list = [widgets.Output() for _ in range(num_widgets)]
    
    # Render content in output widgets
    for i in range(num_widgets):
        with widgets_list[i]:
            disp.display(objects[i].style.set_caption(titles[i]))
    
    # Add CSS styles to distribute free space
    box_layout = Layout(display='flex',
                        flex_flow='row',
                        justify_content='flex-start',
                        width='auto'
                       )
    
    # Create Horizontal Box container
    hbox = widgets.HBox(widgets_list, layout=box_layout)
    
    # Render hbox
    disp.display(hbox)

df1 = pd.DataFrame(df[df["Outcome"] == 0]["Insulin"]).describe()
df2 = pd.DataFrame(df[df["Outcome"] == 1]["Insulin"]).describe()
df3 = pd.DataFrame(df["Insulin"]).describe()

titles = ['Summary of Insulin (Class 0)', 'Summary of Insulin (Class 1)', 'Summary of Insulin (Total)']
render_widgets(3, titles, [df1, df2, df3])


m = round(pd.DataFrame(df[(df["Insulin"] == 0) & (df["Outcome"] == 1)]).shape[0] / df["Outcome"].value_counts()[1], 3)
printmd(f"The ratio of records with missing insulin measurements to the total number of records in the positive class is: {m}", color = "#9DC183")


#Visualize the distribution of Insulin measurements
df["Insulin"].hist(bins = 30)


# 
mean_insulin = int(np.mean(df["Insulin"]))
median_insulin = int(np.median(df["Insulin"]))

df.loc[(df["Outcome"] == 1) & ((df["Insulin"] == 0) | (df["BMI"] > 30)), "Insulin"] = mean_insulin

df.loc[(df["Outcome"] == 0) & (df["Insulin"] == 0), "Insulin"] = median_insulin


df1 = pd.DataFrame(df[df["Outcome"] == 0]["Insulin"]).describe()
df2 = pd.DataFrame(df[df["Outcome"] == 1]["Insulin"]).describe()
df3 = pd.DataFrame(df["Insulin"]).describe()

titles = ['Summary of Insulin (Class 0)', 'Summary of Insulin (Class 1)', 'Summary of Insulin (Total)']
render_widgets(3, titles, [df1, df2, df3])


print(np.median(df[df["Outcome"] == 1]["Insulin"]))


df["Insulin"].hist(bins = 30)


print_missing_data_summary()


df1 = pd.DataFrame(df[df["Outcome"] == 0]["BMI"]).describe()
df2 = pd.DataFrame(df[df["Outcome"] == 1]["BMI"]).describe()
df3 = pd.DataFrame(df["BMI"]).describe()
df4 = pd.DataFrame(df[df["Outcome"] == 0]["BloodPressure"]).describe()
df5 = pd.DataFrame(df[df["Outcome"] == 1]["BloodPressure"]).describe()
df6 = pd.DataFrame(df["BloodPressure"]).describe()

titles = ['Summary of BMI (Class 0)', 'Summary of BMI (Class 1)', 'Summary of BMI (Total)', 'Summary of Blood Pressure (Class 0)', 'Summary of Blood Pressure (Class 1)', 'Summary of Blood Pressure (Total)']
render_widgets(6, titles, [df1, df2, df3, df4, df5, df6])


df.loc[(df["Outcome"] == 1) & (df["BMI"] == 0), "BMI"] = np.median(df["BMI"][df["Outcome"] == 1])

df.loc[(df["Outcome"] == 0) & (df["BMI"] == 0), "BMI"] = np.median(df["BMI"][df["Outcome"] == 0])

df.loc[(df["Outcome"] == 1) & (df["BloodPressure"] == 0), "BloodPressure"] = int(np.median(df["BloodPressure"][df["Outcome"] == 1]))

df.loc[(df["Outcome"] == 0) & (df["BloodPressure"] == 0), "BloodPressure"] = int(np.median(df["BloodPressure"][df["Outcome"] == 0]))


df1 = pd.DataFrame(df[df["Outcome"] == 0]["BMI"]).describe()
df2 = pd.DataFrame(df[df["Outcome"] == 1]["BMI"]).describe()
df3 = pd.DataFrame(df["BMI"]).describe()
df4 = pd.DataFrame(df[df["Outcome"] == 0]["BloodPressure"]).describe()
df5 = pd.DataFrame(df[df["Outcome"] == 1]["BloodPressure"]).describe()
df6 = pd.DataFrame(df["BloodPressure"]).describe()

titles = ['Summary of BMI (Class 0)', 'Summary of BMI (Class 1)', 'Summary of BMI (Total)', 'Summary of Blood Pressure (Class 0)', 'Summary of Blood Pressure (Class 1)', 'Summary of Blood Pressure (Total)']
render_widgets(6, titles, [df1, df2, df3, df4, df5, df6])


print_missing_data_summary()


df1 = pd.DataFrame(df[df["Outcome"] == 0]["Glucose"]).describe()
df2 = pd.DataFrame(df[df["Outcome"] == 1]["Glucose"]).describe()
df3 = pd.DataFrame(df["Glucose"]).describe()
df4 = pd.DataFrame(df[df["Outcome"] == 0]["SkinThickness"]).describe()
df5 = pd.DataFrame(df[df["Outcome"] == 1]["SkinThickness"]).describe()
df6 = pd.DataFrame(df["SkinThickness"]).describe()

titles = ['Summary of Glucose (Class 0)', 'Summary of Glucose (Class 1)', 'Summary of Glucose (Total)', 'Summary of Skin Thickness (Class 0)', 'Summary of Skin Thickness (Class 1)', 'Summary of Skin Thickness (Total)']
render_widgets(6, titles, [df1, df2, df3, df4, df5, df6])


df.loc[(df["Outcome"] == 1) & (df["Glucose"] == 0), "Glucose"] = int(np.median(df["Glucose"][df["Outcome"] == 1]))

df.loc[(df["Outcome"] == 0) & (df["Glucose"] == 0), "Glucose"] = int(np.median(df["Glucose"][df["Outcome"] == 0]))

df.loc[(df["Outcome"] == 1) & (df["SkinThickness"] == 0), "SkinThickness"] = int(np.median(df["SkinThickness"][df["Outcome"] == 1]))

df.loc[(df["Outcome"] == 0) & (df["SkinThickness"] == 0), "SkinThickness"] = int(np.median(df["SkinThickness"][df["Outcome"] == 0]))


df1 = pd.DataFrame(df[df["Outcome"] == 0]["Glucose"]).describe()
df2 = pd.DataFrame(df[df["Outcome"] == 1]["Glucose"]).describe()
df3 = pd.DataFrame(df["Glucose"]).describe()
df4 = pd.DataFrame(df[df["Outcome"] == 0]["SkinThickness"]).describe()
df5 = pd.DataFrame(df[df["Outcome"] == 1]["SkinThickness"]).describe()
df6 = pd.DataFrame(df["SkinThickness"]).describe()

titles = ['Summary of Glucose (Class 0)', 'Summary of Glucose (Class 1)', 'Summary of Glucose (Total)', 'Summary of Skin Thickness (Class 0)', 'Summary of Skin Thickness (Class 1)', 'Summary of Skin Thickness (Total)']
render_widgets(6, titles, [df1, df2, df3, df4, df5, df6])


print_missing_data_summary()


df.hist(bins = 30, figsize = (10, 10), color = 'orange')

plt.show()


# Check the number of records left after cleaning
df.shape


# Create a correlation matrix

corr_matrix = df.corr()

corr_matrix


# Plot the correlation matrix
sns.set_context('talk')

plt.figure(figsize = (10, 10))

_ = sns.heatmap(corr_matrix, annot = True, fmt = ".3f", linewidths = .5)

plt.show()

fig = _.get_figure()

fig.savefig('img/corr_mat_diabetes.png')


## Export the histograms to an image file
# Determine the number of rows and columns for subplots
num_cols = 3
num_rows = -(-len(df.columns) // num_cols)  # Ceiling division to ensure enough rows

# Create a figure and axes
fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 3*num_rows))

# Flatten the axes array for easy iteration
axes = axes.flatten()

# Plot histogram for each column
for i, (col, ax) in enumerate(zip(df.columns, axes)):
    df[col].plot(kind='hist', bins=30, ax=ax, color='orange')
    ax.set_title(col)

# Remove any extra empty subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

# Adjust layout
plt.tight_layout()

# Save the figure
plt.savefig('img/histograms_diabetes.png')

# Show the plot
plt.show()











# split the dataframe into target and features

y = df["Outcome"] # target
X = df.drop(columns = ["Outcome"]) # features

# Verify that the split was performed correctly
print(X.shape)
print(y.shape)


from collections import Counter

counter = Counter(y)
print(counter)


# estimate scale_pos_weight value
estimate = counter[0] / counter[1]
print('Estimate: %.3f' % estimate)


# split the labels and features into training and testing sets

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 21, stratify = y)

# Verify that the split was performed correctly
print('Training set')
print(X_train.shape)
print(y_train.shape)
print()
print('Testing set')
print(X_test.shape)
print(y_test.shape)
print()


# Verify that the index has been shuffled
print(X.index)
print()
print(X_train.index)


from sklearn.preprocessing import RobustScaler

# Create a StandardScaler object
scaler = RobustScaler()

# Fit and transform the training data
X_train_scaled = scaler.fit_transform(X_train)

# Transform the test data (using the same scaling parameters as the training data)
X_test_scaled = scaler.transform(X_test)

display(pd.DataFrame(X_train_scaled).head())

print()

display(X_train.head())





# import the classifier

from xgboost import XGBClassifier


xgb_classifier = XGBClassifier(objective = 'binary:logistic', 
                               eval_metric = 'error', 
                               learning_rate = 0.1,
                               max_depth = 8,
                               alpha = 25,
                               n_estimators = 100,
                               scale_pos_weight=1.908
                               )

xgb_classifier.fit(X_train, y_train)





# predict the performance score of the trained model using the testing dataset

result = xgb_classifier.score(X_test, y_test)
print("Accuracy: {}".format(result))


# make predictions on the test data

y_predict = xgb_classifier.predict(X_test)
y_predict


# print the performance report

from sklearn.metrics import classification_report

print(classification_report(y_test, y_predict))


# print the confusion matrix

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_predict)

sns.set_context('talk')

sns.heatmap(cm, fmt = 'd', annot = True, cmap = 'RdGy')

plt.savefig('img/conf_mat_diabetes.png')






xgb_classifier.fit(X_train_scaled, y_train)

# predict the performance score of the trained model using the testing dataset

result = xgb_classifier.score(X_test_scaled, y_test)
print("Accuracy: {}".format(result))

# make predictions on the test data
y_predict = xgb_classifier.predict(X_test_scaled)

# print the performance report

from sklearn.metrics import classification_report

print(classification_report(y_test, y_predict))

# print the confusion matrix

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_predict)

sns.set_context('talk')

sns.heatmap(cm, fmt = 'd', annot = True, cmap = 'RdGy')

plt.savefig('img/conf_mat_diabetes.png')


from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.metrics import accuracy_score


X_train = X_train.drop(columns = "DiabetesPedigreeFunction") 
X_test = X_test.drop(columns = "DiabetesPedigreeFunction") 


selected_features = SelectKBest(chi2, k = 6).fit(X_train, y_train)

print('Score List: ', selected_features.scores_)
print()
print('Feature list: ', X_train.columns)


X_train_2 = selected_features.transform(X_train)

X_test_2 = selected_features.transform(X_test)

evalset = [(X_train_2, y_train), (X_test_2, y_test)]

xgb_classifier_2 = XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'logloss', 
                                 learning_rate = 0.02,
                                 max_depth = 8,
                                 alpha = 17,
                                 n_estimators = 230,                
                                 min_child_weight = 1,                                 
                                 scale_pos_weight = 1.908,
                                 use_label_encoder = False,
                                 seed = 21).fit(X_train_2, y_train, eval_set = evalset, verbose = 0)

result2 = xgb_classifier_2.score(X_test_2, y_test)
print()
print("Accuracy: {}".format(result2))

print()
print('Accuracy is: ', accuracy_score(y_test, xgb_classifier_2.predict(X_test_2)))
print()

display(pd.DataFrame(X_train_2).head())
display(X_train.head())

cm_2 = confusion_matrix(y_test, xgb_classifier_2.predict(X_test_2))

sns.heatmap(cm_2, annot = True, fmt = 'd')


# stratified k-fold cross validation evaluation of xgboost model
import xgboost
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score

# CV model
model = xgboost.XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'auc', 
                                 learning_rate = 0.0045,
                                 max_depth = 10,
                                 alpha = 17,
                                 n_estimators = 200,                
                                 min_child_weight = 1,                                 
                                 scale_pos_weight = 1.853,
                                 use_label_encoder = False,
                                 seed = 21)
kfold = StratifiedKFold(n_splits=15, shuffle = True, random_state=21)
results = cross_val_score(model, X_train, y_train, cv=kfold)
print("Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))


xgb_classifier_3 = XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'auc', 
                                 learning_rate = 0.0045,
                                 max_depth = 10,
                                 alpha = 17,
                                 n_estimators = 200,                
                                 min_child_weight = 1,                                 
                                 scale_pos_weight = 1.853,
                                 use_label_encoder = False,
                                 seed = 21
                               )

xgb_classifier_3.fit(X_train, y_train)

# predict the performance score of the trained model using the testing dataset

result3 = xgb_classifier_3.score(X_test, y_test)

y_pred_3 = xgb_classifier_3.predict(X_test)


cm_3 = confusion_matrix(y_test, y_pred_3)

sns.heatmap(cm_3, annot = True, fmt = 'd')

print()
print(classification_report(y_test, y_pred_3))

print()
print("Accuracy: {}".format(round(result3, 4)))

print()
# Calculate log loss
log_loss_3 = log_loss(y_test, xgb_classifier_3.predict_proba(X_test))

print("Log Loss: {}".format(round(log_loss_3, 4)))

print()

# Calculate ROC AUC
roc_auc_3 = roc_auc_score(y_test, xgb_classifier_3.predict_proba(X_test)[:, 1])

print("ROC AUC: {}".format(round(roc_auc_3, 4)))

print()


df_less_features = df.drop(columns = ["DiabetesPedigreeFunction"])

# split data into X and y
y_less = df_less_features["Outcome"] # target
X_less = df_less_features.drop(columns = ["Outcome"]) # features

X_less_train, X_less_test, y_less_train, y_less_test = train_test_split(X_less, y_less, test_size = 0.23, random_state = 21, stratify = y)

selected_features = SelectKBest(chi2, k = 6).fit(X_less_train, y_less_train)

print()
print('Score List: ', selected_features.scores_)
print()
print('Feature list: ', X_train.columns)
print()
X_train_4 = selected_features.transform(X_less_train)

X_test_4 = selected_features.transform(X_less_test)

#Compare the transformed and original array
print("Reduced features array")

display(pd.DataFrame(X_train_4).head())

print()
print("Original training set")

display(pd.DataFrame(X_less_train).head())



#Return the transformed arrays to a dataframe with the corresponding column names to keep consistent with the test set
X_train_4 = pd.DataFrame(X_train_4, columns = ["Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "BMI", "Age"])
X_test_4 = pd.DataFrame(X_test_4, columns = ["Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "BMI", "Age"])


xgb_classifier_4 = XGBClassifier(objective = 'binary:logistic',                                   
                                 learning_rate = 0.0045,
                                 max_depth = 10,
                                 alpha = 17,
                                 n_estimators = 200,                
                                 min_child_weight = 1,                                 
                                 scale_pos_weight = 1.853,
                                 use_label_encoder = False,
                                 seed = 21).fit(X_train_4, y_less_train, verbose = 0)

result4 = xgb_classifier_4.score(X_test_4, y_less_test)

y_pred_4 = xgb_classifier_4.predict(X_test_4) 

y_pred_proba_4 = xgb_classifier_4.predict_proba(X_test_4)


# cm_4 = confusion_matrix(y_less_test, y_pred_4)

# sns.heatmap(cm_4, annot = True, fmt = 'd')

print()
print(classification_report(y_less_test, y_pred_4, digits = 4))

print()
print("Accuracy: {}".format(round(result4, 4)))

print()
# Calculate log loss
log_loss_4 = log_loss(y_less_test, y_pred_proba_4)

print("Log Loss: {}".format(round(log_loss_4, 4)))

print()

# Calculate ROC AUC
roc_auc_4 = roc_auc_score(y_less_test, y_pred_proba_4[:, 1])

print("ROC AUC: {}".format(round(roc_auc_4, 4)))

print()

# Calculate Precision-Recall curve
precision, recall, thresholds = precision_recall_curve(y_less_test, y_pred_proba_4[:, 1])

# Define recall levels for interpolation
recall_levels = np.linspace(0, 1, 100)

# Interpolate precision values for each recall level
interp_precision = np.interp(recall_levels, recall[::-1], precision[::-1])

# Calculate AUPRC using the trapezoidal rule
auprc = np.trapz(interp_precision, recall_levels)

print("AUPRC: {}".format(round(auprc, 4)))

print()


from sklearn.model_selection import StratifiedKFold

# Create a StratifiedKFold object with 3 splits
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=21)

X_train_4 = pd.DataFrame(X_train_4)

def strat_objective(trial):    
    # Define the hyperparameter search space
    max_depth = trial.suggest_int('max_depth', 3, 10)
    n_estimators = trial.suggest_int('n_estimators', 100, 1000)
    learning_rate = trial.suggest_float('learning_rate', 0.0, 5e-1, log=False)
    max_leaves = trial.suggest_int('max_leaves', 0, 64)
    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)    
    subsample = trial.suggest_float('subsample', 0.5, 1.0)
    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)
    reg_alpha = trial.suggest_float('reg_alpha', 1e-8, 100, log=False)
    reg_lambda = trial.suggest_float('reg_lambda', 1e-8, 100, log=False)
    scale_pos_weight = trial.suggest_float('scale_pos_weight', 1.853, 1.870)  

    # Create the XGBClassifier with the sampled hyperparameters
    classifier = XGBClassifier(
        max_depth = max_depth,
        n_estimators = n_estimators,
        learning_rate = learning_rate,
        max_leaves = max_leaves,
        min_child_weight=min_child_weight,        
        subsample = subsample,
        colsample_bytree = colsample_bytree,
        reg_alpha = reg_alpha,
        reg_lambda = reg_lambda,
        scale_pos_weight = scale_pos_weight,
        objective = 'binary:logistic',        
        grow_policy = 'lossguide',
        use_label_encoder = False,
        seed = 21,
        n_jobs = -1,        
    )
    
    scores = cross_val_score(classifier, X_train_4, y_less_train, cv=skf, scoring='neg_log_loss', verbose = False, n_jobs=-1)
    return -scores.mean()

# Create an Optuna study
study = optuna.create_study(direction='minimize')

# Optimize the objective function
optuna.logging.set_verbosity(optuna.logging.WARNING)
study.optimize(strat_objective, n_trials=100, timeout = 600, show_progress_bar = True)


# Print the best hyperparameters and the corresponding score
print()
print('Best hyperparameters: ', study.best_params)
print()
print('Best score (Minimum loss): ', study.best_value)
print()


best_params = study.best_params
xgb_classifier_opt = XGBClassifier(**best_params,
                                   objective = 'binary:logistic',
                                   grow_policy = 'lossguide',
                                   use_label_encoder = False,
                                   seed = 21,
                                   n_jobs = -1).fit(X_train_4, y_less_train, verbose = 0)

result_opt = xgb_classifier_opt.score(X_test_4, y_less_test)

y_pred_opt = xgb_classifier_opt.predict(X_test_4) 

y_pred_proba_opt = xgb_classifier_opt.predict_proba(X_test_4)



print()
print(classification_report(y_less_test, y_pred_opt))

print()
print("Accuracy: {}".format(round(result_opt, 4)))

print()
# Calculate log loss
log_loss_opt = log_loss(y_less_test, y_pred_proba_opt)

print("Log Loss: {}".format(round(log_loss_opt, 4)))

print()

# Calculate ROC AUC
roc_auc_opt = roc_auc_score(y_less_test, y_pred_proba_opt[:, 1])

print("ROC AUC: {}".format(round(roc_auc_opt, 4)))

print()

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_less_test, y_pred_proba_opt[:, 1])

# Calculate Precision-Recall curve
precision, recall, thresholds = precision_recall_curve(y_less_test, y_pred_proba_opt[:, 1])

# Define recall levels for interpolation
recall_levels = np.linspace(0, 1, 100)

# Interpolate precision values for each recall level
interp_precision_opt = np.interp(recall_levels, recall[::-1], precision[::-1])

# Calculate AUPRC using the trapezoidal rule
auprc_opt = np.trapz(interp_precision_opt, recall_levels)

print("AUPRC: {}".format(round(auprc_opt, 4)))

print()


from imblearn.over_sampling import SMOTE

smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X_train_4, y_less_train)

xgb_classifier_resampled = XGBClassifier(**best_params,
                                         objective = 'binary:logistic',
                                         grow_policy = 'lossguide',
                                         use_label_encoder = False,
                                         seed = 21,
                                         n_jobs = -1).fit(X_resampled, y_resampled, verbose = 0)

result_resampled = xgb_classifier_resampled.score(X_test_4, y_less_test)

y_pred_resampled = xgb_classifier_resampled.predict(X_test_4) 

y_pred_proba_resampled = xgb_classifier_resampled.predict_proba(X_test_4)

print()
print(classification_report(y_less_test, y_pred_resampled))

print()
print("Accuracy: {}".format(round(result_resampled, 4)))

print()
# Calculate log loss
log_loss_resampled = log_loss(y_less_test, y_pred_proba_resampled)

print("Log Loss: {}".format(round(log_loss_resampled, 4)))

print()

# Calculate ROC AUC
roc_auc_resampled = roc_auc_score(y_less_test, y_pred_proba_resampled[:, 1])

print("ROC AUC: {}".format(round(roc_auc_resampled, 4)))

print()

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_less_test, y_pred_proba_resampled[:, 1])

# Calculate Precision-Recall curve
precision, recall, thresholds = precision_recall_curve(y_less_test, y_pred_proba_resampled[:, 1])

# Define recall levels for interpolation
recall_levels = np.linspace(0, 1, 100)

# Interpolate precision values for each recall level
interp_precision = np.interp(recall_levels, recall[::-1], precision[::-1])

# Calculate AUPRC using the trapezoidal rule
auprc = np.trapz(interp_precision, recall_levels)

print("AUPRC: {}".format(round(auprc, 4)))


import optuna
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score

# Create a StratifiedKFold object with 3 splits
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=21)

# Define the objective function to optimize
def objective(trial):    
    # Define the hyperparameter search space
    max_depth = trial.suggest_int('max_depth', 3, 10)
    n_estimators = trial.suggest_int('n_estimators', 100, 1000)
    learning_rate = trial.suggest_float('learning_rate', 0.0, 5e-1, log=False)
    max_leaves = trial.suggest_int('max_leaves', 0, 64)
    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)    
    subsample = trial.suggest_float('subsample', 0.5, 1.0)
    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)
    reg_alpha = trial.suggest_float('reg_alpha', 1e-8, 100, log=False)
    reg_lambda = trial.suggest_float('reg_lambda', 1e-8, 100, log=False)
    

    # Create the XGBClassifier with the sampled hyperparameters
    classifier = XGBClassifier(
        max_depth=max_depth,
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        max_leaves = max_leaves,
        min_child_weight=min_child_weight,        
        subsample=subsample,
        colsample_bytree=colsample_bytree,
        reg_alpha=reg_alpha,
        reg_lambda=reg_lambda,        
        objective='binary:logistic',        
        grow_policy = 'lossguide',
        use_label_encoder=False,
        seed=21,
        n_jobs=-1,
    )

    # Perform cross-validation and return the negative mean score
    score = cross_val_score(classifier, X_resampled, y_resampled, cv=skf, scoring='neg_log_loss', verbose = False, n_jobs=-1)
    return -score.mean()

# Create an Optuna study
study = optuna.create_study(direction='minimize')

# Optimize the objective function
optuna.logging.set_verbosity(optuna.logging.WARNING)
study.optimize(objective, n_trials=100, timeout = 600, show_progress_bar = True)


# Print the best hyperparameters and the corresponding score
print()
print('Best hyperparameters: ', study.best_params)
print()
print('Best score (Minimum loss): ', study.best_value)
print()


best_params = study.best_params

xgb_classifier_resampled = XGBClassifier(**best_params,
                                         objective = 'binary:logistic', 
                                         grow_policy = 'lossguide',
                                         use_label_encoder=False,
                                         seed=21,
                                         n_jobs=-1,).fit(X_resampled, y_resampled, verbose = 0)

result_resampled = xgb_classifier_resampled.score(X_test_4, y_less_test)

y_pred_resampled = xgb_classifier_resampled.predict(X_test_4) 

y_pred_proba_resampled = xgb_classifier_resampled.predict_proba(X_test_4)


print()
print(classification_report(y_less_test, y_pred_resampled))

print()
print("Accuracy: {}".format(round(result_resampled, 4)))

print()
# Calculate log loss
log_loss_resampled = log_loss(y_less_test, y_pred_proba_resampled)

print("Log Loss: {}".format(round(log_loss_resampled, 4)))

print()

# Calculate ROC AUC
roc_auc_resampled = roc_auc_score(y_less_test, y_pred_proba_resampled[:, 1])

print("ROC AUC: {}".format(round(roc_auc_resampled, 4)))

print()

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_less_test, y_pred_proba_resampled[:, 1])

# Calculate Precision-Recall curve
precision, recall, thresholds = precision_recall_curve(y_less_test, y_pred_proba_resampled[:, 1])

# Define recall levels for interpolation
recall_levels = np.linspace(0, 1, 100)

# Interpolate precision values for each recall level
interp_precision = np.interp(recall_levels, recall[::-1], precision[::-1])

# Calculate AUPRC using the trapezoidal rule
auprc = np.trapz(interp_precision, recall_levels)

print("AUPRC: {}".format(round(auprc, 4)))

print()


from hyperopt import hp, fmin, tpe, STATUS_OK, Trials
from sklearn.model_selection import cross_val_score

# Define the hyperparameter search space
space = {
    'max_depth': hp.quniform('max_depth', 2, 10, 1),
    'n_estimators': hp.quniform('n_estimators', 100, 1000, 1),
    'learning_rate': hp.uniform('learning_rate', 1e-3, 5e-1),
    'max_leaves': hp.choice('max_leaves', np.arange(10, 65, dtype = int)),
    'subsample': hp.uniform('subsample', 0.5, 1.0),
    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),    
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),
    'reg_alpha': hp.uniform('reg_alpha', 1e-8, 1.0),
    'reg_lambda': hp.uniform('reg_lambda', 1e-8, 1.0),
}

# Define the objective function to optimize
def objective(params):
    classifier = XGBClassifier(
        max_depth = int(params['max_depth']),
        n_estimators = int(params['n_estimators']),
        learning_rate = params['learning_rate'],
        max_leaves = int(params['max_leaves']),
        min_child_weight = int(params['min_child_weight']),        
        subsample = params['subsample'],
        colsample_bytree = params['colsample_bytree'],
        reg_alpha = params['reg_alpha'],
        reg_lambda = params['reg_lambda'],
        objective = 'binary:logistic',        
        use_label_encoder = False,
        seed = 21,
        n_jobs = -1,
    )

    score = cross_val_score(classifier, X_train_4, y_less_train, cv=5, scoring='neg_log_loss', n_jobs=-1)
    return {'loss': -score.mean(), 'status': STATUS_OK}

# Create a Trials object to track the optimization process
trials = Trials()

# Optimize the objective function using fmin
best = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,
    max_evals=100,
    trials=trials,
    rstate=np.random.default_rng(21)
)


# Print the best hyperparameters and the corresponding score
print()
print("Best hyperparameters:", best)
print()
print("Best score:", trials.best_trial['result']['loss'])


# Retrieve the best hyperparameters
best_params = trials.best_trial['misc']['vals']

# Extract scalar values from the lists
max_depth = int(best_params['max_depth'][0])
n_estimators = int(best_params['n_estimators'][0])
learning_rate = best_params['learning_rate'][0]
min_child_weight = int(best_params['min_child_weight'][0])
max_leaves = best_params['max_leaves'][0]
colsample_bytree = best_params['colsample_bytree'][0]
subsample = best_params['subsample'][0]
reg_alpha = best_params['reg_alpha'][0]
reg_lambda = best_params['reg_lambda'][0]

xgb_classifier_hyp = XGBClassifier(objective = 'binary:logistic',                                     
                                   learning_rate = learning_rate,
                                   max_depth = max_depth,
                                   max_leaves = max_leaves,
                                   n_estimators = n_estimators,                
                                   min_child_weight = min_child_weight,
                                   subsample = subsample,
                                   colsample_bytree = colsample_bytree,
                                   reg_alpha = reg_alpha,
                                   reg_lambda = reg_lambda,                                 
                                   scale_pos_weight = 1.853,
                                   use_label_encoder = False,
                                   seed = 21).fit(X_train_4, y_less_train, verbose = 0)

result_hyp = xgb_classifier_hyp.score(X_test_4, y_less_test)

y_pred_hyp = xgb_classifier_hyp.predict(X_test_4) 

y_pred_proba_hyp = xgb_classifier_hyp.predict_proba(X_test_4)


print()
print(classification_report(y_less_test, y_pred_hyp, digits = 4))

print()
print("Accuracy: {}".format(round(result_hyp, 4)))

print()
# Calculate log loss
log_loss_hyp = log_loss(y_less_test, y_pred_proba_hyp)

print("Log Loss: {}".format(round(log_loss_hyp, 4)))

print()

# Calculate ROC AUC
roc_auc_hyp = roc_auc_score(y_less_test, y_pred_proba_hyp[:, 1])

print("ROC AUC: {}".format(round(roc_auc_hyp, 4)))

print()

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_less_test, y_pred_proba_hyp[:, 1])

# Calculate Precision-Recall curve
precision, recall, thresholds = precision_recall_curve(y_less_test, y_pred_proba_hyp[:, 1])

# Define recall levels for interpolation
recall_levels = np.linspace(0, 1, 100)

# Interpolate precision values for each recall level
interp_precision = np.interp(recall_levels, recall[::-1], precision[::-1])

# Calculate AUPRC using the trapezoidal rule
auprc = np.trapz(interp_precision, recall_levels)

print("AUPRC: {}".format(round(auprc, 4)))

print()


import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import average_precision_score

# LightGBM with Stratified K-Fold Cross-Validation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=21)

# Define the objective function for LightGBM
def lgb_objective(trial):
    params = {        
        'objective': 'binary',        
        'max_depth': trial.suggest_int('max_depth', 2, 10),
        'num_leaves': trial.suggest_int('num_leaves', 10, 64),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 5e-1, log=False),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 11),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=False),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=False),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'random_state': 21,
        'n_jobs': -1, 
        'verbosity': -1,
    }

    scores = cross_val_score(lgb.LGBMClassifier(**params), X_train_4, y_less_train, cv=skf, scoring='neg_log_loss', n_jobs=-1)
    return -scores.mean()

# Create an Optuna study
study = optuna.create_study(direction='minimize')

# Optimize the objective function
optuna.logging.set_verbosity(optuna.logging.WARNING)
study.optimize(lgb_objective, n_trials=100, timeout = 600, show_progress_bar = True)


# Print the best hyperparameters and the corresponding score
print()
print("Best hyperparameters:", study.best_params)
print()
print("Best score (Minimum loss):", study.best_value)
print()


# Train the LightGBM model with the best hyperparameters
best_params = study.best_params
lgb_model = lgb.LGBMClassifier(**best_params, verbosity = -1)
lgb_model.fit(X_train_4, y_less_train)

# Evaluate the model on the test set
result_lgb = lgb_model.score(X_test_4, y_less_test)
y_pred_lgb = lgb_model.predict(X_test_4) 
y_pred_proba_lgb = lgb_model.predict_proba(X_test_4)[:, 1]
roc_auc_lgb = roc_auc_score(y_less_test, y_pred_proba_lgb)
auprc_lgb = average_precision_score(y_less_test, y_pred_proba_lgb)

print()
print(classification_report(y_less_test, y_pred_lgb, digits = 4))

print()
print("Accuracy: {}".format(round(result_lgb, 4)))

# Calculate log loss
log_loss_lgb = log_loss(y_less_test, y_pred_proba_lgb)
print()
print("Log Loss: {}".format(round(log_loss_lgb, 4)))
print()
print("ROC AUC (LightGBM):", round(roc_auc_lgb, 4))
print()
print("AUPRC (LightGBM):", round(auprc_lgb, 4))


import lightgbm as lgb
import warnings
from hyperopt import hp, fmin, tpe, STATUS_OK, Trials
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import average_precision_score
import sklearn

# LightGBM with Stratified K-Fold Cross-Validation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=21)

# Define the hyperparameter search space
space = {        
    'max_depth': hp.choice('max_depth', np.arange(2, 11, dtype = int)),
    'n_estimators': hp.choice('n_estimators', np.arange(100, 1001, dtype = int)),
    'learning_rate': hp.uniform('learning_rate', 1e-3, 5e-1),
    'min_child_weight': hp.choice('min_child_weight', np.arange(1, 11, dtype = int)),
    'num_leaves': hp.choice('num_leaves', np.arange(10, 65, dtype = int)),
    'subsample': hp.uniform('subsample', 0.5, 1.0),
    'min_child_samples': hp.choice('min_child_samples', np.arange(5, 101, dtype = int)),
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),
    'reg_alpha': hp.uniform('reg_alpha', 1e-8, 1.0),
    'reg_lambda': hp.uniform('reg_lambda', 1e-8, 1.0),
}

# Define the objective function to optimize
def objective(params):    
    classifier = lgb.LGBMClassifier(        
        objective = 'binary',
        max_depth = int(params['max_depth']),
        n_estimators = int(params['n_estimators']),
        learning_rate = params['learning_rate'],
        min_child_weight = int(params['min_child_weight']),        
        subsample = params['subsample'],
        colsample_bytree = params['colsample_bytree'],
        reg_alpha = params['reg_alpha'],
        reg_lambda = params['reg_lambda'],
        is_unbalance = True,                
        seed = 21,
        n_jobs = -1,
        verbosity = -1,
    )
    
    score = cross_val_score(classifier, X_train_4, y_less_train, cv=skf, scoring='neg_log_loss', n_jobs=-1)
    return {'loss': -score.mean(), 'status': STATUS_OK}

# Create a Trials object to track the optimization process
trials = Trials()

# Optimize the objective function using fmin
best = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,
    max_evals=100,
    trials=trials,
    rstate=np.random.default_rng(21)
)


# Print the best hyperparameters and the corresponding score
print()
print("Best hyperparameters:", best)
print()
print("Best score (Minimum loss):", trials.best_trial['result']['loss'])


# Retrieve the best hyperparameters
best_params = trials.best_trial['misc']['vals']

# Extract scalar values from the lists
max_depth = int(best_params['max_depth'][0])
n_estimators = int(best_params['n_estimators'][0])
learning_rate = best_params['learning_rate'][0]
min_child_weight = int(best_params['min_child_weight'][0])
min_child_samples = int(best_params['min_child_samples'][0])
num_leaves = int(best_params['num_leaves'][0]) 
colsample_bytree = best_params['colsample_bytree'][0]
subsample = best_params['subsample'][0]
reg_alpha = best_params['reg_alpha'][0]
reg_lambda = best_params['reg_lambda'][0]


lgb_classifier_hyp = lgb.LGBMClassifier(objective = 'binary',
                                        max_depth = max_depth,
                                        n_estimators = n_estimators,
                                        learning_rate = learning_rate,
                                        min_child_weight = min_child_weight,
                                        min_child_samples = min_child_samples,
                                        num_eaves = num_leaves,
                                        colsample_bytree = colsample_bytree,
                                        subsample = subsample,
                                        reg_alpha = reg_alpha,
                                        reg_lambda = reg_lambda,                    
                                        is_unbalance = True,                                      
                                        seed=21,
                                        n_jobs=-1, 
                                        verbosity = -1).fit(X_train_4, y_less_train)
        
result_hyp = lgb_classifier_hyp.score(X_test_4, y_less_test)

y_pred_hyp = lgb_classifier_hyp.predict(X_test_4) 

y_pred_proba_hyp = lgb_classifier_hyp.predict_proba(X_test_4)


print()
print(classification_report(y_less_test, y_pred_hyp, digits = 4))

print()
print("Accuracy: {}".format(round(result_hyp, 4)))

print()
# Calculate log loss
log_loss_hyp = log_loss(y_less_test, y_pred_proba_hyp)

print("Log Loss: {}".format(round(log_loss_hyp, 4)))

print()

# Calculate ROC AUC
roc_auc_hyp = roc_auc_score(y_less_test, y_pred_proba_hyp[:, 1])

print("ROC AUC: {}".format(round(roc_auc_hyp, 4)))

print()

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_less_test, y_pred_proba_hyp[:, 1])

# Calculate Precision-Recall curve
precision, recall, thresholds = precision_recall_curve(y_less_test, y_pred_proba_hyp[:, 1])

# Define recall levels for interpolation
recall_levels = np.linspace(0, 1, 100)

# Interpolate precision values for each recall level
interp_precision = np.interp(recall_levels, recall[::-1], precision[::-1])

# Calculate AUPRC using the trapezoidal rule
auprc = np.trapz(interp_precision, recall_levels)

print("AUPRC: {}".format(round(auprc, 4)))

print() 


from catboost import CatBoostClassifier
from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score
import optuna
import catboost
from optuna_integration import CatBoostPruningCallback


# Consider StratifiedShuffleSplit for imbalanced classes
skf = StratifiedShuffleSplit(n_splits=5, random_state=21)

# Define the objective function with early stopping
def catboost_objective(trial):
    params = {
        'objective': 'Logloss',  
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0),
        'depth': trial.suggest_int('max_depth', 2, 10), 
        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 5e-1, log=False),        
        'num_leaves': trial.suggest_int('num_leaves', 10, 64),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 100),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 100.0, log=False),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'random_strength': trial.suggest_int('random_strength', 1, 100),
        'subsample': trial.suggest_float("subsample", 0.5, 1.0, log=False),
        'random_seed': 21,
        'grow_policy': 'Lossguide',
        'boosting_type': 'Plain',
        'used_ram_limit': '4gb',        
        'silent':True,
        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.852, 1.870)        
    }

    scores = cross_val_score(CatBoostClassifier(**params), X_train_4, y_less_train, cv=skf, scoring='neg_log_loss', n_jobs=-1)
    return -scores.mean()

    

# Create an Optuna study
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),
                            direction='minimize')

# Optimize the objective function
optuna.logging.set_verbosity(optuna.logging.WARNING)
study.optimize(catboost_objective, n_trials=100, timeout=600, show_progress_bar = True)


# Print the best hyperparameters and the corresponding score
print()
print("Best hyperparameters:", study.best_params)
print()
print("Best score (Minimum loss):", study.best_value)
print()


# Train the CatBoost model with the best hyperparameters
best_params = study.best_params
catboost_model = CatBoostClassifier(**best_params,
                                    objective = 'Logloss',
                                    grow_policy = 'Lossguide',
                                    silent = True,
                                    boosting_type = 'Plain',
                                    random_seed = 21,                                    
)

catboost_model.fit(X_train_4, y_less_train)

# Evaluate the model on the test set
result_cat = catboost_model.score(X_test_4, y_less_test)
y_pred_cat = catboost_model.predict(X_test_4) 
y_pred_proba_cat = catboost_model.predict_proba(X_test_4)[:, 1]
roc_auc_cat = roc_auc_score(y_less_test, y_pred_proba_cat)
auprc_cat = average_precision_score(y_less_test, y_pred_proba_cat)

print()
print(classification_report(y_less_test, y_pred_cat, digits = 4))
print()
print("Accuracy (CatBoost): {}".format(round(result_cat, 4)))

# Calculate log loss
log_loss_cat = log_loss(y_less_test, y_pred_proba_cat)
print()
print("Log Loss (CatBoost): {}".format(round(log_loss_cat, 4)))
print()
print("ROC AUC (CatBoost):", roc_auc_cat)
print()
print("AUPRC (CatBoost):", auprc_cat)


from catboost import CatBoostClassifier
from hyperopt import hp, fmin, tpe, STATUS_OK, Trials
from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score
import numpy as np
import sklearn

# Consider StratifiedShuffleSplit for imbalanced classes
skf = StratifiedShuffleSplit(n_splits=5, random_state=21)

# Define the hyperparameter search space
space = {
    'depth': hp.quniform('depth', 2, 10, 1), 
    'n_estimators': hp.choice('n_estimators', np.arange(100, 1001, dtype = int)),
    'learning_rate': hp.uniform('learning_rate', 0.0, 1.0),
    'max_leaves': hp.uniform('max_leaves', 10, 64),
    'l2_leaf_reg': hp.uniform('l2_leaf_reg', 0, 100),
    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1.0),
    'min_data_in_leaf': hp.uniform("min_data_in_leaf", 5, 100), 
    'subsample': hp.uniform('subsample', 0.5, 1.0),
    'random_strength': hp.quniform('random_strength', 1, 100, 1),
    }

# Define the objective function with early stopping
def catboost_objective(params):
    classifier = CatBoostClassifier(        
        objective = 'Logloss',        
        depth = int(params['depth']),
        iterations = int(params['n_estimators']),
        learning_rate = params['learning_rate'],
        max_leaves = int(params['max_leaves']),
        l2_leaf_reg = params['l2_leaf_reg'],
        colsample_bylevel = params['colsample_bylevel'],
        min_data_in_leaf = params['min_data_in_leaf'],
        subsample = params['subsample'],
        random_strength = int(params['random_strength']),        
        random_state = 21,
        grow_policy = 'Lossguide',
        boosting_type = 'Plain',
        silent = True,
        thread_count = -1,  # Enable GPU if available

    )

    score = cross_val_score(classifier, X_train_4, y_less_train, cv=skf, scoring='neg_log_loss', n_jobs=-1)
    return {'loss': -score.mean(), 'status': STATUS_OK}
    
# Create a Trials object to track the optimization process
trials = Trials()

# Optimize the objective function using fmin
best = fmin(
    fn=catboost_objective,
    space=space,
    algo=tpe.suggest,
    max_evals=100,
    trials=trials,
    rstate=np.random.default_rng(21)
)


# Print the best hyperparameters and the corresponding score
print()
print("Best hyperparameters:", best)
print()
print("Best score:", trials.best_trial['result']['loss'])


# Retrieve the best hyperparameters
best_params = trials.best_trial['misc']['vals']

# Extract scalar values from the lists
colsample_bylevel = best_params['colsample_bylevel'][0]
depth = int(best_params['depth'][0])
learning_rate = best_params['learning_rate'][0]
max_leaves = int(best_params['max_leaves'][0])
min_data_in_leaf = int(best_params['min_data_in_leaf'][0])
l2_leaf_reg = best_params['l2_leaf_reg'][0]
n_estimators = best_params['n_estimators'][0]
random_strength = int(best_params['random_strength'][0])
subsample = best_params['subsample'][0]


# Create a new CatBoostClassifier instance with the best hyperparameters
best_model = CatBoostClassifier(
    objective='Logloss',
    colsample_bylevel=colsample_bylevel,
    depth=depth,
    learning_rate=learning_rate,
    max_leaves = max_leaves,
    min_data_in_leaf = min_data_in_leaf,
    l2_leaf_reg=l2_leaf_reg,
    n_estimators = n_estimators,    
    random_strength=random_strength,
    subsample=subsample,
    random_state=21,
    grow_policy = 'Lossguide',
    boosting_type = 'Plain',
    silent = True,
    thread_count=-1,    
)

# Train the best model on the resampled data
best_model.fit(X_train_4, y_less_train)

# Evaluate the model on the test set
result_catboost = best_model.score(X_test_4, y_less_test)
y_pred_catboost = best_model.predict(X_test_4)
y_pred_proba_catboost = best_model.predict_proba(X_test_4)[:, 1]
roc_auc_catboost = roc_auc_score(y_less_test, y_pred_proba_catboost)
auprc_catboost = average_precision_score(y_less_test, y_pred_proba_catboost)

print()
print(classification_report(y_less_test, y_pred_catboost, digits = 4))
print()
print("Accuracy (CatBoost): {}".format(round(result_catboost, 4)))

# Calculate log loss
log_loss_catboost = log_loss(y_less_test, y_pred_proba_catboost)
print()
print("Log Loss (CatBoost): {}".format(round(log_loss_catboost, 4)))
print()
print("ROC AUC (CatBoost):", roc_auc_catboost)
print()
print("AUPRC (CatBoost):", auprc_catboost)



