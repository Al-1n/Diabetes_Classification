import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


df = pd.read_csv("./Data/diabetes.csv", sep = ",")

df.head()


df.tail()





df.describe()


df.hist(bins = 30, figsize = (20, 20), color = 'orange')

plt.show()





df = df[(df["BloodPressure"] > 0) & (df["BMI"] > 0) & (df["Glucose"] > 0)]

df.hist(bins = 30, figsize = (20, 20), color = 'orange')

plt.show()


# Check the number of records left after cleaning
df.shape


## Export the histograms to an image file
# Determine the number of rows and columns for subplots
num_cols = 3
num_rows = -(-len(df.columns) // num_cols)  # Ceiling division to ensure enough rows

# Create a figure and axes
fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, 5*num_rows))

# Flatten the axes array for easy iteration
axes = axes.flatten()

# Plot histogram for each column
for i, (col, ax) in enumerate(zip(df.columns, axes)):
    df[col].plot(kind='hist', bins=30, ax=ax, color='orange')
    ax.set_title(col)

# Remove any extra empty subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

# Adjust layout
plt.tight_layout()

# Save the figure
plt.savefig('img/histograms_diabetes.png')

# Show the plot
plt.show()


# Create a correlation matrix

corr_matrix = df.corr()

corr_matrix





# Plot the correlation matrix

plt.figure(figsize = (16, 16))

_ = sns.heatmap(corr_matrix, annot = True, fmt = ".3f", linewidths = .5)

plt.show()

fig = _.get_figure()

fig.savefig('img/corr_mat_diabetes.png')








# split the dataframe into target and features

y = df["Outcome"] # target
X = df.drop(columns = ["Outcome"]) # features

# Verify that the split was performed correctly
print(X.shape)
print(y.shape)


from collections import Counter

counter = Counter(y)
print(counter)


# estimate scale_pos_weight value
estimate = counter[0] / counter[1]
print('Estimate: %.3f' % estimate)


# split the labels and features into training and testing sets

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 21, stratify = y)

# Verify that the split was performed correctly
print('Training set')
print(X_train.shape)
print(y_train.shape)
print()
print('Testing set')
print(X_test.shape)
print(y_test.shape)
print()


# Verify that the index has been shuffled
print(X.index)
print()
print(X_train.index)





# import the classifier

from xgboost import XGBClassifier


xgb_classifier = XGBClassifier(objective = 'binary:logistic', 
                               eval_metric = 'error', 
                               learning_rate = 0.1,
                               max_depth = 8,
                               alpha = 25,
                               n_estimators = 100,
                               scale_pos_weight=1.908
                               )

xgb_classifier.fit(X_train, y_train)





# predict the performance score of the trained model using the testing dataset

result = xgb_classifier.score(X_test, y_test)
print("Accuracy: {}".format(result))


# make predictions on the test data

y_predict = xgb_classifier.predict(X_test)
y_predict


# print the performance report

from sklearn.metrics import classification_report

print(classification_report(y_test, y_predict))


# print the confusion matrix

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_predict)
sns.heatmap(cm, fmt = 'd', annot = True, cmap = 'RdGy')

plt.savefig('img/conf_mat_diabetes.png')






from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.metrics import accuracy_score


X_train = X_train.drop(columns = "DiabetesPedigreeFunction") 
X_test = X_test.drop(columns = "DiabetesPedigreeFunction") 


selected_features = SelectKBest(chi2, k = 6).fit(X_train, y_train)

print('Score List: ', selected_features.scores_)
print()
print('Feature list: ', X_train.columns)


X_train_2 = selected_features.transform(X_train)

X_test_2 = selected_features.transform(X_test)

evalset = [(X_train_2, y_train), (X_test_2, y_test)]

xgb_classifier_2 = XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'logloss', 
                                 learning_rate = 0.02,
                                 max_depth = 8,
                                 alpha = 17,
                                 n_estimators = 230,                
                                 min_child_weight = 1,                                 
                                 scale_pos_weight = 1.908,
                                 use_label_encoder = False,
                                 seed = 21).fit(X_train_2, y_train, eval_set = evalset, verbose = 0)

result2 = xgb_classifier_2.score(X_test_2, y_test)
print()
print("Accuracy: {}".format(result2))

print()
print('Accuracy is: ', accuracy_score(y_test, xgb_classifier_2.predict(X_test_2)))
print()

cm_2 = confusion_matrix(y_test, xgb_classifier_2.predict(X_test_2))

sns.heatmap(cm_2, annot = True, fmt = 'd')


results = xgb_classifier_2.evals_result()

plt.plot(results['validation_0']['logloss'], label = 'train')
plt.plot(results['validation_1']['logloss'], label = 'test')

plt.xlabel('Number of trees')
plt.ylabel('Error')

plt.legend()

plt.show()


print(result2)





from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score

# CV model

kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state = 21)
results = cross_val_score(xgb_classifier_2, X, y, cv=kfold)
print("Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))


# stratified k-fold cross validation evaluation of xgboost model
from numpy import loadtxt
import xgboost
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score

# load data
df = pd.read_csv('./Data/diabetes.csv')

df = df[(df["BloodPressure"] > 0) & (df["BMI"] > 0) & (df["Glucose"] > 0)]

df = df.drop(columns = ["DiabetesPedigreeFunction", "Pregnancies", "BloodPressure"])

# split data into X and y
y = df["Outcome"] # target
X = df.drop(columns = ["Outcome"]) # features

# CV model
model = xgboost.XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'logloss', 
                                 learning_rate = 0.0045,
                                 max_depth = 10,
                                 alpha = 17,
                                 n_estimators = 200,                
                                 min_child_weight = 1,                                 
                                 scale_pos_weight = 1.853,
                                 use_label_encoder = False,
                                 seed = 21)
kfold = StratifiedKFold(n_splits=15, shuffle = True, random_state=7)
results = cross_val_score(model, X, y, cv=kfold)
print("Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))


# load data
df = pd.read_csv('./Data/diabetes.csv')

df = df[(df["BloodPressure"] > 0) & (df["BMI"] > 0) & (df["Glucose"] > 0)]

df = df.drop(columns = ["DiabetesPedigreeFunction", "Pregnancies"])

# split data into X and y
y = df["Outcome"] # target
X = df.drop(columns = ["Outcome"]) # features

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 21, stratify = y)

xgb_classifier_3 = XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'logloss', 
                                 learning_rate = 0.0045,
                                 max_depth = 10,
                                 alpha = 17,
                                 n_estimators = 200,                
                                 min_child_weight = 1,                                 
                                 scale_pos_weight = 1.853,
                                 use_label_encoder = False,
                                 seed = 21
                               )

xgb_classifier_3.fit(X_train, y_train)

# predict the performance score of the trained model using the testing dataset

result3 = xgb_classifier_3.score(X_test, y_test)
print("Accuracy: {}".format(result3))

print(classification_report(y_test, y_predict))

cm_3 = confusion_matrix(y_test, xgb_classifier_3.predict(X_test))

sns.heatmap(cm_3, annot = True, fmt = 'd')


# load data
df = pd.read_csv('./Data/diabetes.csv')

df = df[(df["BloodPressure"] > 0) & (df["BMI"] > 0) & (df["Glucose"] > 0)]

df = df.drop(columns = ["DiabetesPedigreeFunction", "Pregnancies", "BloodPressure"])

# split data into X and y
y = df["Outcome"] # target
X = df.drop(columns = ["Outcome"]) # features

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.23, random_state = 21, stratify = y)

selected_features = SelectKBest(chi2, k = 4).fit(X_train, y_train)

print('Score List: ', selected_features.scores_)
print()
print('Feature list: ', X_train.columns)

X_train_2 = selected_features.transform(X_train)

X_test_2 = selected_features.transform(X_test)

evalset = [(X_train_2, y_train), (X_test_2, y_test)]

xgb_classifier_2 = XGBClassifier(objective = 'binary:logistic', 
                                 eval_metric = 'logloss', 
                                 learning_rate = 0.0045,
                                 max_depth = 10,
                                 alpha = 17,
                                 n_estimators = 200,                
                                 min_child_weight = 1,                                 
                                 scale_pos_weight = 1.853,
                                 use_label_encoder = False,
                                 seed = 21).fit(X_train_2, y_train, eval_set = evalset, verbose = 0)

result2 = xgb_classifier_2.score(X_test_2, y_test)
print()
print("Accuracy: {}".format(result2))

print()
print(classification_report(y_test, xgb_classifier_2.predict(X_test_2)))

cm_2 = confusion_matrix(y_test, xgb_classifier_2.predict(X_test_2))

sns.heatmap(cm_2, annot = True, fmt = 'd')


from sklearn.metrics import log_loss, roc_auc_score

# Calculate log loss
print(log_loss(y_test, xgb_classifier_2.predict_proba(X_test_2)))

print()

# Calculate ROC AUC
print(roc_auc_score(y_test, xgb_classifier_2.predict_proba(X_test_2)[:, 1]))



